{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982ecf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, VarianceThreshold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabbeacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"training_data_75.csv\"\n",
    "SEED = 42\n",
    "TEST_SIZE_FILES = 0.25\n",
    "\n",
    "LOW_MAX_THR = 2  # low = 1-2, high = 4-64\n",
    "\n",
    "N_SPLITS_GATE = 5\n",
    "N_SPLITS_LOW  = 5\n",
    "N_SPLITS_HIGH = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69ec99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 137 | Unique files: 36\n",
      "Num features: 64\n",
      "Thr counts: {np.int64(1): np.int64(54), np.int64(2): np.int64(46), np.int64(4): np.int64(6), np.int64(8): np.int64(17), np.int64(16): np.int64(12), np.int64(64): np.int64(2)}\n",
      "Gate counts: {np.int64(0): np.int64(100), np.int64(1): np.int64(37)}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "y_thr = df[\"min_threshold\"].astype(int).values\n",
    "y_gate = (y_thr > LOW_MAX_THR).astype(int)\n",
    "groups = df[\"file\"].astype(str).values\n",
    "\n",
    "drop_cols = [\n",
    "    \"min_threshold\", \"file\", \"family\",\n",
    "    \"forward_runtime\", \"max_fidelity_achieved\",\n",
    "    \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\",\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "\n",
    "X_df = df.drop(columns=drop_cols).copy()\n",
    "\n",
    "print(\"Rows:\", len(df), \"| Unique files:\", df[\"file\"].nunique())\n",
    "print(\"Num features:\", X_df.shape[1])\n",
    "print(\"Thr counts:\", dict(zip(*np.unique(y_thr, return_counts=True))))\n",
    "print(\"Gate counts:\", dict(zip(*np.unique(y_gate, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bce5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 103 | Test rows: 34\n",
      "Train files: 27 | Test files: 9\n",
      "Overlap files: 0\n",
      "Gate test counts: {np.int64(0): np.int64(24), np.int64(1): np.int64(10)}\n",
      "Thr  test counts: {np.int64(1): np.int64(14), np.int64(2): np.int64(10), np.int64(8): np.int64(4), np.int64(16): np.int64(4), np.int64(64): np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "def stratified_file_split_by_gate(df, test_size=0.25, seed=42, low_max_thr=2):\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    file_info = df.groupby(\"file\", as_index=False).agg(\n",
    "        thr=(\"min_threshold\", \"first\"),\n",
    "        n_rows=(\"min_threshold\", \"size\")\n",
    "    )\n",
    "    file_info[\"gate\"] = (file_info[\"thr\"].astype(int) > low_max_thr).astype(int)\n",
    "\n",
    "    low_files  = file_info.loc[file_info[\"gate\"] == 0, \"file\"].astype(str).tolist()\n",
    "    high_files = file_info.loc[file_info[\"gate\"] == 1, \"file\"].astype(str).tolist()\n",
    "\n",
    "    rng.shuffle(low_files)\n",
    "    rng.shuffle(high_files)\n",
    "\n",
    "    n_low_test  = max(1, int(round(len(low_files) * test_size)))  if len(low_files)  > 1 else 1\n",
    "    n_high_test = max(1, int(round(len(high_files) * test_size))) if len(high_files) > 1 else 1\n",
    "\n",
    "    test_files  = set(low_files[:n_low_test] + high_files[:n_high_test])\n",
    "    train_files = set(file_info[\"file\"].astype(str)) - test_files\n",
    "\n",
    "    train_idx = df.index[df[\"file\"].astype(str).isin(train_files)].to_numpy()\n",
    "    test_idx  = df.index[df[\"file\"].astype(str).isin(test_files)].to_numpy()\n",
    "\n",
    "    return train_idx, test_idx, train_files, test_files\n",
    "\n",
    "train_idx, test_idx, train_files, test_files = stratified_file_split_by_gate(\n",
    "    df, test_size=TEST_SIZE_FILES, seed=SEED, low_max_thr=LOW_MAX_THR\n",
    ")\n",
    "\n",
    "print(\"Train rows:\", len(train_idx), \"| Test rows:\", len(test_idx))\n",
    "print(\"Train files:\", len(train_files), \"| Test files:\", len(test_files))\n",
    "print(\"Overlap files:\", len(train_files.intersection(test_files)))\n",
    "print(\"Gate test counts:\", dict(zip(*np.unique(y_gate[test_idx], return_counts=True))))\n",
    "print(\"Thr  test counts:\", dict(zip(*np.unique(y_thr[test_idx], return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621ff73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features (train): 64\n",
      "Unique train files: 27\n",
      "Unique test  files: 9\n"
     ]
    }
   ],
   "source": [
    "split = {\n",
    "    \"X_train\": X_df.iloc[train_idx].reset_index(drop=True),\n",
    "    \"X_test\":  X_df.iloc[test_idx].reset_index(drop=True),\n",
    "    \"y_train\": y_thr[train_idx].astype(int),\n",
    "    \"y_test\":  y_thr[test_idx].astype(int),\n",
    "    \"gate_train\": y_gate[train_idx].astype(int),\n",
    "    \"gate_test\":  y_gate[test_idx].astype(int),\n",
    "    \"groups_train\": df.loc[train_idx, \"file\"].astype(str).values,\n",
    "    \"groups_test\":  df.loc[test_idx, \"file\"].astype(str).values,\n",
    "}\n",
    "\n",
    "print(\"Num features (train):\", split[\"X_train\"].shape[1])\n",
    "print(\"Unique train files:\", len(np.unique(split[\"groups_train\"])))\n",
    "print(\"Unique test  files:\", len(np.unique(split[\"groups_test\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8357cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate train: (103, 64) | Gate test: (34, 64)\n",
      "Low  train: (76, 64) | low classes: [np.int64(1), np.int64(2)]\n",
      "High train: (27, 64) | high classes: [np.int64(4), np.int64(8), np.int64(16)]\n"
     ]
    }
   ],
   "source": [
    "X_train = split[\"X_train\"]\n",
    "X_test  = split[\"X_test\"]\n",
    "\n",
    "y_thr_train  = split[\"y_train\"]\n",
    "y_thr_test   = split[\"y_test\"]\n",
    "y_gate_train = split[\"gate_train\"]\n",
    "y_gate_test  = split[\"gate_test\"]\n",
    "\n",
    "groups_train = split[\"groups_train\"]\n",
    "groups_test  = split[\"groups_test\"]\n",
    "\n",
    "# gate subset\n",
    "Xg_tr, yg_tr, gg_tr = X_train, y_gate_train, groups_train\n",
    "Xg_te, yg_te, gg_te = X_test,  y_gate_test,  groups_test\n",
    "\n",
    "# low subset\n",
    "low_tr_mask = (y_thr_train <= LOW_MAX_THR)\n",
    "low_te_mask = (y_thr_test  <= LOW_MAX_THR)\n",
    "\n",
    "Xl_tr = X_train.loc[low_tr_mask].reset_index(drop=True)\n",
    "yl_tr = y_thr_train[low_tr_mask]\n",
    "gl_tr = groups_train[low_tr_mask]\n",
    "\n",
    "Xl_te = X_test.loc[low_te_mask].reset_index(drop=True)\n",
    "yl_te = y_thr_test[low_te_mask]\n",
    "gl_te = groups_test[low_te_mask]\n",
    "\n",
    "# high subset\n",
    "high_tr_mask = (y_thr_train > LOW_MAX_THR)\n",
    "high_te_mask = (y_thr_test  > LOW_MAX_THR)\n",
    "\n",
    "Xh_tr = X_train.loc[high_tr_mask].reset_index(drop=True)\n",
    "yh_tr = y_thr_train[high_tr_mask]\n",
    "gh_tr = groups_train[high_tr_mask]\n",
    "\n",
    "Xh_te = X_test.loc[high_te_mask].reset_index(drop=True)\n",
    "yh_te = y_thr_test[high_te_mask]\n",
    "gh_te = groups_test[high_te_mask]\n",
    "\n",
    "subsets = {\n",
    "    \"gate\": (Xg_tr, yg_tr, gg_tr, Xg_te, yg_te, gg_te),\n",
    "    \"low\":  (Xl_tr, yl_tr, gl_tr, Xl_te, yl_te, gl_te),\n",
    "    \"high\": (Xh_tr, yh_tr, gh_tr, Xh_te, yh_te, gh_te),\n",
    "}\n",
    "\n",
    "print(\"Gate train:\", Xg_tr.shape, \"| Gate test:\", Xg_te.shape)\n",
    "print(\"Low  train:\", Xl_tr.shape, \"| low classes:\", sorted(np.unique(yl_tr)))\n",
    "print(\"High train:\", Xh_tr.shape, \"| high classes:\", sorted(np.unique(yh_tr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8970e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_scalar(y_true, y_pred):\n",
    "    y_true = int(y_true); y_pred = int(y_pred)\n",
    "    return 0.0 if y_pred < y_true else float(y_true) / float(y_pred)\n",
    "\n",
    "def mean_reward(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    return float(np.mean([reward_scalar(t, p) for t, p in zip(y_true, y_pred)]))\n",
    "\n",
    "def reward_matrix(classes):\n",
    "    classes = np.asarray(classes, dtype=int)\n",
    "    R = np.zeros((len(classes), len(classes)), dtype=float)\n",
    "    for i, t in enumerate(classes):\n",
    "        for j, p in enumerate(classes):\n",
    "            R[i, j] = reward_scalar(t, p)\n",
    "    return R\n",
    "\n",
    "def predict_max_expected_reward(proba_aligned, classes):\n",
    "    classes = np.asarray(classes, dtype=int)\n",
    "    R = reward_matrix(classes)\n",
    "    exp_reward = proba_aligned @ R\n",
    "    best_j = np.argmax(exp_reward, axis=1)\n",
    "    return classes[best_j]\n",
    "\n",
    "def align_proba_to_classes(estimator, proba, classes_all):\n",
    "    classes_all = np.asarray(classes_all, dtype=int)\n",
    "    est_classes = np.asarray(estimator.classes_, dtype=int)\n",
    "\n",
    "    out = np.zeros((proba.shape[0], len(classes_all)), dtype=float)\n",
    "    col_map = {c: i for i, c in enumerate(est_classes)}\n",
    "\n",
    "    for j, c in enumerate(classes_all):\n",
    "        if c in col_map:\n",
    "            out[:, j] = proba[:, col_map[c]]\n",
    "\n",
    "    row_sums = out.sum(axis=1, keepdims=True)\n",
    "    mask = row_sums.squeeze() > 0\n",
    "    out[mask] = out[mask] / row_sums[mask]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16416e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X):\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or str(X[c].dtype).startswith(\"category\")]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1932b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_selector(score_func=\"mi\", k_best=None):\n",
    "    if k_best is None:\n",
    "        return None\n",
    "    if score_func == \"mi\":\n",
    "        return SelectKBest(mutual_info_classif, k=k_best)\n",
    "    if score_func == \"f\":\n",
    "        return SelectKBest(f_classif, k=k_best)\n",
    "    raise ValueError(\"score_func must be 'mi' or 'f'\")\n",
    "\n",
    "def model_lr_binary(X_ref, C=1.0, k_best=10, score_func=\"mi\"):\n",
    "    pre = build_preprocessor(X_ref)\n",
    "    sel = build_selector(score_func=score_func, k_best=k_best)\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        C=C, solver=\"liblinear\", max_iter=8000, random_state=SEED\n",
    "    )\n",
    "\n",
    "    steps = [(\"pre\", pre), (\"vt\", VarianceThreshold())]\n",
    "    if sel is not None:\n",
    "        steps.append((\"sel\", sel))\n",
    "    steps.append((\"clf\", lr))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "def model_et(X_ref, n_estimators=900, max_depth=None, min_samples_leaf=1, k_best=15, score_func=\"f\"):\n",
    "    pre = build_preprocessor(X_ref)\n",
    "    sel = build_selector(score_func=score_func, k_best=k_best)\n",
    "\n",
    "    et = ExtraTreesClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "    steps = [(\"pre\", pre), (\"vt\", VarianceThreshold())]\n",
    "    if sel is not None:\n",
    "        steps.append((\"sel\", sel))\n",
    "    steps.append((\"clf\", et))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "def model_lr_ovr_multiclass(X_ref, C=0.01, k_best=None, score_func=\"mi\"):\n",
    "    pre = build_preprocessor(X_ref)\n",
    "    sel = build_selector(score_func=score_func, k_best=k_best)\n",
    "\n",
    "    base = LogisticRegression(\n",
    "        C=C, solver=\"liblinear\", max_iter=12000, random_state=SEED\n",
    "    )\n",
    "    ovr = OneVsRestClassifier(base)\n",
    "\n",
    "    steps = [(\"pre\", pre), (\"vt\", VarianceThreshold())]\n",
    "    if sel is not None:\n",
    "        steps.append((\"sel\", sel))\n",
    "    steps.append((\"clf\", ovr))\n",
    "    return Pipeline(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0a7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folds(X, y, groups, n_splits):\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    return [(tr, te) for tr, te in gkf.split(X, y, groups=groups)]\n",
    "\n",
    "def eval_gate_cv(estimator, X, y_gate, groups, folds):\n",
    "    baccs = []\n",
    "    fn_rates = []\n",
    "    fp_rates = []\n",
    "\n",
    "    for tr_idx, te_idx in folds:\n",
    "        est = clone(estimator)\n",
    "        est.fit(X.iloc[tr_idx], y_gate[tr_idx])\n",
    "\n",
    "        y_pred = est.predict(X.iloc[te_idx])\n",
    "        y_true = y_gate[te_idx]\n",
    "\n",
    "        true_high = (y_true == 1).sum()\n",
    "        true_low  = (y_true == 0).sum()\n",
    "        fn = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "        fp = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "\n",
    "        fn_rate = fn / true_high if true_high else 0.0\n",
    "        fp_rate = fp / true_low  if true_low else 0.0\n",
    "\n",
    "        baccs.append(balanced_accuracy_score(y_true, y_pred))\n",
    "        fn_rates.append(fn_rate)\n",
    "        fp_rates.append(fp_rate)\n",
    "\n",
    "    return float(np.mean(baccs)), float(np.std(baccs)), float(np.max(fn_rates)), float(np.mean(fp_rates))\n",
    "\n",
    "def eval_threshold_cv_reward(estimator, X, y_thr, groups, folds, classes_ladder):\n",
    "    scores = []\n",
    "    classes_ladder = np.asarray(classes_ladder, dtype=int)\n",
    "\n",
    "    for tr_idx, te_idx in folds:\n",
    "        est = clone(estimator)\n",
    "        est.fit(X.iloc[tr_idx], y_thr[tr_idx])\n",
    "\n",
    "        proba = est.predict_proba(X.iloc[te_idx])\n",
    "        proba_aligned = align_proba_to_classes(est, proba, classes_ladder)\n",
    "\n",
    "        y_pred = predict_max_expected_reward(proba_aligned, classes_ladder)\n",
    "        scores.append(mean_reward(y_thr[te_idx], y_pred))\n",
    "\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9789bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "c:\\Users\\Hiram\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2924: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top gate (name, mean_bacc, std, fn_max, fp_mean):\n",
      "('gate_lr_C3.0_k20_mi', 0.7150000000000001, 0.19209372712298547, 1.0, 0.09)\n",
      "('gate_lr_C1.0_k20_mi', 0.6900000000000001, 0.19078784028338913, 1.0, 0.13999999999999999)\n",
      "('gate_lr_C0.1_k20_mi', 0.6483333333333333, 0.28869437896232836, 1.0, 0.22333333333333333)\n",
      "('gate_lr_C3.0_k10_mi', 0.5349999999999999, 0.14106735979665885, 1.0, 0.09)\n",
      "('gate_lr_C1.0_k10_mi', 0.4766666666666667, 0.18844392033470092, 1.0, 0.20666666666666664)\n",
      "('gate_lr_C0.1_k10_mi', 0.44333333333333325, 0.2225109235770485, 1.0, 0.2733333333333333)\n",
      "\n",
      "Top low (name, mean_reward, std):\n",
      "('low_et_n400_dNone_k30_f', 0.8916666666666666, 0.0565194165260439)\n",
      "('low_et_n400_d8_k30_f', 0.8916666666666666, 0.0565194165260439)\n",
      "('low_et_n900_dNone_k30_f', 0.8916666666666666, 0.0565194165260439)\n",
      "('low_et_n900_d8_k30_f', 0.8916666666666666, 0.0565194165260439)\n",
      "('low_et_n900_dNone_k15_f', 0.8916666666666666, 0.097182531580755)\n",
      "('low_et_n900_d8_k15_f', 0.8916666666666666, 0.097182531580755)\n",
      "('low_et_n400_dNone_k15_f', 0.8666666666666666, 0.08079466429027216)\n",
      "('low_et_n400_d8_k15_f', 0.8666666666666666, 0.08079466429027216)\n",
      "('low_et_n400_dNone_k10_f', 0.8166666666666667, 0.12527746981977425)\n",
      "('low_et_n400_d8_k10_f', 0.8166666666666667, 0.12527746981977425)\n",
      "\n",
      "Top high (name, mean_reward, std):\n",
      "('high_lr_ovr_C0.1_k10_mi', 0.7785714285714286, 0.08001913036574314)\n",
      "('high_lr_ovr_C0.01_kNone_mi', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.01_kNone_f', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.01_k10_mi', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.01_k10_f', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.01_k20_mi', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.01_k20_f', 0.6803571428571429, 0.1842230652092784)\n",
      "('high_lr_ovr_C0.001_kNone_mi', 0.5803571428571428, 0.1414551742656684)\n",
      "('high_lr_ovr_C0.001_kNone_f', 0.5803571428571428, 0.1414551742656684)\n",
      "('high_lr_ovr_C0.001_k10_mi', 0.5803571428571428, 0.1414551742656684)\n",
      "\n",
      "Selected winners:\n",
      "Gate: gate_lr_C3.0_k20_mi\n",
      "Low : low_et_n400_dNone_k30_f\n",
      "High: high_lr_ovr_C0.1_k10_mi\n"
     ]
    }
   ],
   "source": [
    "# --- CV folds ---\n",
    "gate_folds = make_folds(Xg_tr, yg_tr, gg_tr, N_SPLITS_GATE)\n",
    "low_folds  = make_folds(Xl_tr, yl_tr, gl_tr, N_SPLITS_LOW)\n",
    "high_folds = make_folds(Xh_tr, yh_tr, gh_tr, N_SPLITS_HIGH)\n",
    "\n",
    "# --- Manual tiny grids ---\n",
    "gate_grid = []\n",
    "for C in [0.1, 1.0, 3.0]:\n",
    "    for k in [10, 20]:\n",
    "        name = f\"gate_lr_C{C}_k{k}_mi\"\n",
    "        est = model_lr_binary(Xg_tr, C=C, k_best=k, score_func=\"mi\")\n",
    "        mean_bacc, std_bacc, fn_max, fp_mean = eval_gate_cv(est, Xg_tr, yg_tr, gg_tr, gate_folds)\n",
    "        gate_grid.append((name, mean_bacc, std_bacc, fn_max, fp_mean))\n",
    "\n",
    "gate_grid = sorted(gate_grid, key=lambda x: (x[3], -x[1], x[4]))\n",
    "print(\"Top gate (name, mean_bacc, std, fn_max, fp_mean):\")\n",
    "for r in gate_grid[:10]:\n",
    "    print(r)\n",
    "best_gate_name = gate_grid[0][0]\n",
    "\n",
    "low_classes = np.array([1,2], dtype=int)\n",
    "low_grid = []\n",
    "for n in [400, 900]:\n",
    "    for d in [None, 8]:\n",
    "        for k in [10, 15, 30]:\n",
    "            name = f\"low_et_n{n}_d{d}_k{k}_f\"\n",
    "            est = model_et(Xl_tr, n_estimators=n, max_depth=d, k_best=k, score_func=\"f\")\n",
    "            mean_r, std_r = eval_threshold_cv_reward(est, Xl_tr, yl_tr, gl_tr, low_folds, low_classes)\n",
    "            low_grid.append((name, mean_r, std_r))\n",
    "\n",
    "low_grid = sorted(low_grid, key=lambda x: (-x[1], x[2]))\n",
    "print(\"\\nTop low (name, mean_reward, std):\")\n",
    "for r in low_grid[:10]:\n",
    "    print(r)\n",
    "best_low_name = low_grid[0][0]\n",
    "\n",
    "high_classes = np.array([4,8,16,64], dtype=int)\n",
    "high_grid = []\n",
    "for C in [0.001, 0.01, 0.1, 1.0]:\n",
    "    for k in [None, 10, 20]:\n",
    "        for sf in [\"mi\", \"f\"]:\n",
    "            name = f\"high_lr_ovr_C{C}_k{k}_{sf}\"\n",
    "            est = model_lr_ovr_multiclass(Xh_tr, C=C, k_best=k, score_func=sf)\n",
    "            mean_r, std_r = eval_threshold_cv_reward(est, Xh_tr, yh_tr, gh_tr, high_folds, high_classes)\n",
    "            high_grid.append((name, mean_r, std_r))\n",
    "\n",
    "high_grid = sorted(high_grid, key=lambda x: (-x[1], x[2]))\n",
    "print(\"\\nTop high (name, mean_reward, std):\")\n",
    "for r in high_grid[:10]:\n",
    "    print(r)\n",
    "best_high_name = high_grid[0][0]\n",
    "\n",
    "print(\"\\nSelected winners:\")\n",
    "print(\"Gate:\", best_gate_name)\n",
    "print(\"Low :\", best_low_name)\n",
    "print(\"High:\", best_high_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164be411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed winners:\n",
      "Gate: gate_lr_C3.0_k20_mi | C= 3.0 k= 20\n",
      "Low : low_et_n400_dNone_k30_f | n= 400 d= None k= 30\n",
      "High: high_lr_ovr_C0.1_k10_mi | C= 0.1 k= 10 sf= mi\n",
      "Models fitted.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_gate(name):\n",
    "    # example: gate_lr_C1.0_k10_mi\n",
    "    m = re.match(r\"^gate_lr_C(?P<C>[-+0-9.eE]+)_k(?P<k>\\d+)_(?P<sf>mi|f)$\", name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad gate name format: {name}\")\n",
    "    C = float(m.group(\"C\"))\n",
    "    k = int(m.group(\"k\"))\n",
    "    return C, k\n",
    "\n",
    "def parse_low(name):\n",
    "    # example: low_et_n900_dNone_k15_f  OR low_et_n400_d8_k10_f\n",
    "    m = re.match(r\"^low_et_n(?P<n>\\d+)_d(?P<d>None|\\d+)_k(?P<k>\\d+)_(?P<sf>mi|f)$\", name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad low name format: {name}\")\n",
    "    n = int(m.group(\"n\"))\n",
    "    d_raw = m.group(\"d\")\n",
    "    d = None if d_raw == \"None\" else int(d_raw)\n",
    "    k = int(m.group(\"k\"))\n",
    "    return n, d, k\n",
    "\n",
    "def parse_high(name):\n",
    "    # example: high_lr_ovr_C0.01_kNone_mi  OR high_lr_ovr_C1.0_k20_f\n",
    "    m = re.match(r\"^high_lr_ovr_C(?P<C>[-+0-9.eE]+)_k(?P<k>None|\\d+)_(?P<sf>mi|f)$\", name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad high name format: {name}\")\n",
    "    C = float(m.group(\"C\"))\n",
    "    k_raw = m.group(\"k\")\n",
    "    k = None if k_raw == \"None\" else int(k_raw)\n",
    "    sf = m.group(\"sf\")\n",
    "    return C, k, sf\n",
    "\n",
    "# ---- parse winners ----\n",
    "Cg, Kg = parse_gate(best_gate_name)\n",
    "Nl, Dl, Kl = parse_low(best_low_name)\n",
    "Ch, Kh, Sh = parse_high(best_high_name)\n",
    "\n",
    "# ---- build winners ----\n",
    "gate_best = model_lr_binary(Xg_tr, C=Cg, k_best=Kg, score_func=\"mi\")\n",
    "low_best  = model_et(Xl_tr, n_estimators=Nl, max_depth=Dl, k_best=Kl, score_func=\"f\")\n",
    "high_best = model_lr_ovr_multiclass(Xh_tr, C=Ch, k_best=Kh, score_func=Sh)\n",
    "\n",
    "# ---- fit ----\n",
    "gate_best.fit(Xg_tr, yg_tr)\n",
    "low_best.fit(Xl_tr, yl_tr)\n",
    "high_best.fit(Xh_tr, yh_tr)\n",
    "\n",
    "print(\"Parsed winners:\")\n",
    "print(\"Gate:\", best_gate_name, \"| C=\", Cg, \"k=\", Kg)\n",
    "print(\"Low :\", best_low_name,  \"| n=\", Nl, \"d=\", Dl, \"k=\", Kl)\n",
    "print(\"High:\", best_high_name, \"| C=\", Ch, \"k=\", Kh, \"sf=\", Sh)\n",
    "print(\"Models fitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a6ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_predict_thresholds(gate_model, low_model, high_model, X_test, y_gate_true, y_thr_true):\n",
    "    gate_pred = gate_model.predict(X_test).astype(int)\n",
    "\n",
    "    y_pred = np.zeros(len(X_test), dtype=int)\n",
    "    route = np.array([\"LOW\"] * len(X_test), dtype=object)\n",
    "    route[gate_pred == 1] = \"HIGH\"\n",
    "\n",
    "    low_idx = np.where(gate_pred == 0)[0]\n",
    "    if len(low_idx) > 0:\n",
    "        proba = low_model.predict_proba(X_test.iloc[low_idx])\n",
    "        proba = align_proba_to_classes(low_model, proba, np.array([1,2], dtype=int))\n",
    "        y_pred[low_idx] = predict_max_expected_reward(proba, np.array([1,2], dtype=int))\n",
    "\n",
    "    high_idx = np.where(gate_pred == 1)[0]\n",
    "    if len(high_idx) > 0:\n",
    "        proba = high_model.predict_proba(X_test.iloc[high_idx])\n",
    "        proba = align_proba_to_classes(high_model, proba, np.array([4,8,16,64], dtype=int))\n",
    "        y_pred[high_idx] = predict_max_expected_reward(proba, np.array([4,8,16,64], dtype=int))\n",
    "\n",
    "    dbg = pd.DataFrame({\n",
    "        \"gate_true\": y_gate_true.astype(int),\n",
    "        \"gate_pred\": gate_pred.astype(int),\n",
    "        \"route\": route,\n",
    "        \"thr_true\": y_thr_true.astype(int),\n",
    "        \"thr_pred\": y_pred.astype(int),\n",
    "    })\n",
    "    return y_pred, dbg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54afffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HOLDOUT RESULTS (seed = 42 ) ===\n",
      "Test rows: 34 | Unique test files: 9\n",
      "Mean reward: 0.7058823529411765\n",
      "Zero-rate: 0.23529411764705882\n",
      "Mean TRUE low: 0.9166666666666666\n",
      "Mean TRUE high: 0.2\n",
      "\n",
      "Gate confusion matrix (rows true [low,high] x cols pred [low,high]):\n",
      " [[24  0]\n",
      " [ 4  6]]\n",
      "\n",
      "Worst 10 rows:\n",
      " gate_true  gate_pred route  thr_true  thr_pred  reward\n",
      "         0          0   LOW         2         1     0.0\n",
      "         0          0   LOW         2         1     0.0\n",
      "         1          1  HIGH        64        16     0.0\n",
      "         1          1  HIGH        64        16     0.0\n",
      "         1          0   LOW        16         1     0.0\n",
      "         1          0   LOW        16         1     0.0\n",
      "         1          0   LOW        16         1     0.0\n",
      "         1          0   LOW        16         1     0.0\n",
      "         1          1  HIGH         8        16     0.5\n",
      "         1          1  HIGH         8        16     0.5\n"
     ]
    }
   ],
   "source": [
    "y_pred_test, dbg = pipeline_predict_thresholds(\n",
    "    gate_best, low_best, high_best,\n",
    "    X_test, y_gate_test, y_thr_test\n",
    ")\n",
    "\n",
    "dbg[\"reward\"] = [reward_scalar(t, p) for t, p in zip(dbg[\"thr_true\"], dbg[\"thr_pred\"])]\n",
    "\n",
    "cm = confusion_matrix(dbg[\"gate_true\"], dbg[\"gate_pred\"], labels=[0,1])\n",
    "\n",
    "print(\"\\n=== HOLDOUT RESULTS (seed =\", SEED, \") ===\")\n",
    "print(\"Test rows:\", len(dbg), \"| Unique test files:\", len(np.unique(groups_test)))\n",
    "print(\"Mean reward:\", float(np.mean(dbg[\"reward\"].values)))\n",
    "print(\"Zero-rate:\", float(np.mean(dbg[\"reward\"].values == 0.0)))\n",
    "print(\"Mean TRUE low:\", float(dbg.loc[dbg[\"gate_true\"]==0, \"reward\"].mean()) if (dbg[\"gate_true\"]==0).any() else None)\n",
    "print(\"Mean TRUE high:\", float(dbg.loc[dbg[\"gate_true\"]==1, \"reward\"].mean()) if (dbg[\"gate_true\"]==1).any() else None)\n",
    "print(\"\\nGate confusion matrix (rows true [low,high] x cols pred [low,high]):\\n\", cm)\n",
    "\n",
    "print(\"\\nWorst 10 rows:\")\n",
    "print(dbg.sort_values(\"reward\").head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ba4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MULTI-SEED SUMMARY ===\n",
      "seed= 42 | mean_reward= 0.5955882352941176 | zero_rate= 0.23529411764705882 | gate_fn= 0.0 | gate_fp= 0.16666666666666666 | test_files= 9\n",
      "gate_cm:\n",
      " [[20  4]\n",
      " [ 0 10]] \n",
      "\n",
      "seed= 7 | mean_reward= 0.7291666666666666 | zero_rate= 0.1111111111111111 | gate_fn= 0.0 | gate_fp= 0.16666666666666666 | test_files= 9\n",
      "gate_cm:\n",
      " [[20  4]\n",
      " [ 0 12]] \n",
      "\n",
      "seed= 99 | mean_reward= 0.6857142857142857 | zero_rate= 0.2 | gate_fn= 0.6363636363636364 | gate_fp= 0.0 | test_files= 9\n",
      "gate_cm:\n",
      " [[24  0]\n",
      " [ 7  4]] \n",
      "\n",
      "Mean over seeds: 0.67015639589169\n",
      "Std  over seeds: 0.05563173724764618\n",
      "Max  zero_rate : 0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "def run_once(seed):\n",
    "    train_idx, test_idx, _, _ = stratified_file_split_by_gate(\n",
    "        df, test_size=TEST_SIZE_FILES, seed=seed, low_max_thr=LOW_MAX_THR\n",
    "    )\n",
    "\n",
    "    X_train = X_df.iloc[train_idx].reset_index(drop=True)\n",
    "    X_test  = X_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    y_thr_train = y_thr[train_idx].astype(int)\n",
    "    y_thr_test  = y_thr[test_idx].astype(int)\n",
    "    y_gate_train = (y_thr_train > LOW_MAX_THR).astype(int)\n",
    "    y_gate_test  = (y_thr_test  > LOW_MAX_THR).astype(int)\n",
    "\n",
    "    groups_train = df.loc[train_idx, \"file\"].astype(str).values\n",
    "    groups_test  = df.loc[test_idx, \"file\"].astype(str).values\n",
    "\n",
    "    # subsets\n",
    "    low_tr_mask = (y_thr_train <= LOW_MAX_THR)\n",
    "    high_tr_mask = (y_thr_train > LOW_MAX_THR)\n",
    "\n",
    "    Xg_tr, yg_tr, gg_tr = X_train, y_gate_train, groups_train\n",
    "    Xl_tr, yl_tr, gl_tr = X_train.loc[low_tr_mask].reset_index(drop=True), y_thr_train[low_tr_mask], groups_train[low_tr_mask]\n",
    "    Xh_tr, yh_tr, gh_tr = X_train.loc[high_tr_mask].reset_index(drop=True), y_thr_train[high_tr_mask], groups_train[high_tr_mask]\n",
    "\n",
    "    # rebuild winners using parsed params (same as CELL 11)\n",
    "    gate_m = model_lr_binary(Xg_tr, C=Cg, k_best=Kg, score_func=\"mi\")\n",
    "    low_m  = model_et(Xl_tr, n_estimators=Nl, max_depth=Dl, k_best=Kl, score_func=\"f\")\n",
    "    high_m = model_lr_ovr_multiclass(Xh_tr, C=Ch, k_best=Kh, score_func=Sh)\n",
    "\n",
    "    gate_m.fit(Xg_tr, yg_tr)\n",
    "    low_m.fit(Xl_tr, yl_tr)\n",
    "    high_m.fit(Xh_tr, yh_tr)\n",
    "\n",
    "    y_pred, dbg = pipeline_predict_thresholds(gate_m, low_m, high_m, X_test, y_gate_test, y_thr_test)\n",
    "    dbg[\"reward\"] = [reward_scalar(t, p) for t, p in zip(dbg[\"thr_true\"], dbg[\"thr_pred\"])]\n",
    "\n",
    "    mean_reward = float(np.mean(dbg[\"reward\"].values))\n",
    "    zero_rate = float(np.mean(dbg[\"reward\"].values == 0.0))\n",
    "\n",
    "    cm = confusion_matrix(dbg[\"gate_true\"], dbg[\"gate_pred\"], labels=[0,1])\n",
    "    true_high = (dbg[\"gate_true\"] == 1).sum()\n",
    "    true_low  = (dbg[\"gate_true\"] == 0).sum()\n",
    "    fn_rate = float(((dbg[\"gate_true\"]==1) & (dbg[\"gate_pred\"]==0)).sum() / true_high) if true_high else 0.0\n",
    "    fp_rate = float(((dbg[\"gate_true\"]==0) & (dbg[\"gate_pred\"]==1)).sum() / true_low)  if true_low else 0.0\n",
    "\n",
    "    return {\"seed\": seed, \"mean_reward\": mean_reward, \"zero_rate\": zero_rate, \"gate_fn\": fn_rate, \"gate_fp\": fp_rate, \"test_files\": len(np.unique(groups_test)), \"gate_cm\": cm}\n",
    "\n",
    "seeds = [42, 7, 99]\n",
    "res = [run_once(s) for s in seeds]\n",
    "\n",
    "print(\"\\n=== MULTI-SEED SUMMARY ===\")\n",
    "for r in res:\n",
    "    print(\"seed=\", r[\"seed\"], \"| mean_reward=\", r[\"mean_reward\"], \"| zero_rate=\", r[\"zero_rate\"],\n",
    "          \"| gate_fn=\", r[\"gate_fn\"], \"| gate_fp=\", r[\"gate_fp\"], \"| test_files=\", r[\"test_files\"])\n",
    "    print(\"gate_cm:\\n\", r[\"gate_cm\"], \"\\n\")\n",
    "\n",
    "print(\"Mean over seeds:\", float(np.mean([r[\"mean_reward\"] for r in res])))\n",
    "print(\"Std  over seeds:\", float(np.std([r[\"mean_reward\"] for r in res])))\n",
    "print(\"Max  zero_rate :\", float(np.max([r[\"zero_rate\"] for r in res])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
