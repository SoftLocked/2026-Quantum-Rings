{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3590207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce872204",
   "metadata": {},
   "source": [
    "**Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0f7b0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (102, 66) (35, 66)\n",
      "Train classes: [np.int64(1), np.int64(2), np.int64(4), np.int64(8), np.int64(16), np.int64(64)]\n",
      "Test classes: [np.int64(1), np.int64(2), np.int64(4)]\n",
      "Unique files train: 27 test: 9 overlap: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# read dataset\n",
    "df = pd.read_csv(\"training_data_75.csv\")\n",
    "\n",
    "# Target for classification is the threshold value\n",
    "y = df[\"min_threshold\"].astype(int).values\n",
    "\n",
    "# Group (so all rows of same circuit stay together)\n",
    "groups = df[\"file\"].astype(str).values\n",
    "\n",
    "# Drop columns we dont want as features\n",
    "drop_cols = [\n",
    "    \"min_threshold\",   # target\n",
    "    \"file\",\n",
    "    \"family\",\n",
    "    \"forward_runtime\", # not for classification, only regression\n",
    "    \"max_fidelity_achieved\",\n",
    "    \"forward_shots\",\n",
    "    \"forward_peak_rss_mb\",\n",
    "    \"n_thresholds_tested\",\n",
    "]\n",
    "drop_cols = [c for c in drop_cols if c in df.columns]\n",
    "\n",
    "# X is equal to the whole dataset - dropped columns\n",
    "X = df.drop(columns=drop_cols).copy()\n",
    "\n",
    "# encode categorical columns (backend/precision/etc.)\n",
    "X = pd.get_dummies(X, columns=[c for c in X.columns if X[c].dtype == \"object\" or X[c].dtype == \"str\"])\n",
    "\n",
    "# ---------------------------\n",
    "# Stratified split BY FILE, stratified by n_qubits bucket\n",
    "# ---------------------------\n",
    "\n",
    "# 1) Build a file-level table for stratification\n",
    "file_info = df.groupby(\"file\", as_index=False).agg(\n",
    "    n_qubits=(\"n_qubits\", \"first\")\n",
    ")\n",
    "\n",
    "# Bucketize n_qubits so stratification is stable (avoids classes with only 1 file)\n",
    "file_info[\"qubit_bucket\"] = pd.cut(\n",
    "    file_info[\"n_qubits\"],\n",
    "    bins=[-1, 20, 60, 10**9],\n",
    "    labels=[\"small\", \"medium\", \"large\"]\n",
    ")\n",
    "\n",
    "# 2) Optional: force rare-threshold files into TRAIN (helps avoid \"unseen class 256\")\n",
    "forced_train_files = set(df.loc[df[\"min_threshold\"] == 256, \"file\"].unique())\n",
    "\n",
    "# Split only on remaining files (pool)\n",
    "pool = file_info[~file_info[\"file\"].isin(forced_train_files)].reset_index(drop=True)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "train_f_idx, test_f_idx = next(sss.split(pool[\"file\"], pool[\"qubit_bucket\"]))\n",
    "\n",
    "train_files = set(pool.loc[train_f_idx, \"file\"])\n",
    "test_files  = set(pool.loc[test_f_idx, \"file\"])\n",
    "\n",
    "# Add forced files to train\n",
    "train_files |= forced_train_files\n",
    "\n",
    "# Convert file sets -> row indices\n",
    "train_idx = df.index[df[\"file\"].isin(train_files)].to_numpy()\n",
    "test_idx  = df.index[df[\"file\"].isin(test_files)].to_numpy()\n",
    "\n",
    "# Final arrays\n",
    "x_train = X.iloc[train_idx].values.astype(np.float32)\n",
    "x_test  = X.iloc[test_idx].values.astype(np.float32)\n",
    "y_train = y[train_idx]\n",
    "y_test  = y[test_idx]\n",
    "\n",
    "# sanity checks\n",
    "print(\"Shapes:\", x_train.shape, x_test.shape)\n",
    "print(\"Train classes:\", sorted(np.unique(y_train)))\n",
    "print(\"Test classes:\", sorted(np.unique(y_test)))\n",
    "\n",
    "overlap = train_files.intersection(test_files)\n",
    "print(\"Unique files train:\", len(train_files), \"test:\", len(test_files), \"overlap:\", len(overlap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07521eb0",
   "metadata": {},
   "source": [
    "**Metricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "44b52e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cls_metrics(y_true, y_pred, name=\"model\"):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    under = np.mean(y_pred < y_true)   # super importante en tu reto\n",
    "    over  = np.mean(y_pred > y_true)\n",
    "\n",
    "    #print(f\"{name}\")\n",
    "    #print(\"  Accuracy:\", round(acc, 4))\n",
    "    #print(\"  Under-rate (pred < true):\", round(float(under), 4))\n",
    "    #print(\"  Over-rate  (pred > true):\", round(float(over), 4))\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57914913-f3e5-4d23-b128-50bbeada9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL MODEL: RandomForest + Top 10 Features + Conservative Prediction\n",
      "======================================================================\n",
      "Threshold classes: [np.int64(1), np.int64(2), np.int64(4), np.int64(8), np.int64(16), np.int64(64)]\n",
      "\n",
      "Top 10 features (by CatBoost importance):\n",
      "   1. n_h\n",
      "   2. avg_gate_span\n",
      "   3. cx_ratio\n",
      "   4. n_cx\n",
      "   5. n_u2\n",
      "   6. crude_depth\n",
      "   7. std_gate_span\n",
      "   8. ratio_1q_gates\n",
      "   9. 1q_gates_per_qubit\n",
      "  10. depth_squared\n",
      "\n",
      "Evaluation: 2-fold StratifiedGroupKFold (grouped by file)\n",
      "\n",
      "======================================================================\n",
      "STRATEGY COMPARISON (Minimizing Underprediction Risk)\n",
      "======================================================================\n",
      "\n",
      "raw\n",
      "  Score: 99.0/137 = 0.7226 per sample\n",
      "  Exact:  94 ( 68.6%) â†’ 94 pts\n",
      "  Under:  28 ( 20.4%) â†’ 0 pts [RISK!]\n",
      "  Over:   15 ( 10.9%) â†’ 5.0 pts\n",
      "\n",
      "conservative_60\n",
      "  Score: 96.0/137 = 0.7007 per sample\n",
      "  Exact:  78 ( 56.9%) â†’ 78 pts\n",
      "  Under:  14 ( 10.2%) â†’ 0 pts [RISK!]\n",
      "  Over:   45 ( 32.8%) â†’ 18.0 pts\n",
      "\n",
      "conservative_50\n",
      "  Score: 94.0/137 = 0.6861 per sample\n",
      "  Exact:  82 ( 59.9%) â†’ 82 pts\n",
      "  Under:  24 ( 17.5%) â†’ 0 pts [RISK!]\n",
      "  Over:   31 ( 22.6%) â†’ 12.0 pts\n",
      "\n",
      "conservative_40\n",
      "  Score: 95.0/137 = 0.6934 per sample\n",
      "  Exact:  86 ( 62.8%) â†’ 86 pts\n",
      "  Under:  28 ( 20.4%) â†’ 0 pts [RISK!]\n",
      "  Over:   23 ( 16.8%) â†’ 9.0 pts\n",
      "\n",
      "======================================================================\n",
      "SUMMARY - Sorted by Underprediction Risk (lowest first)\n",
      "======================================================================\n",
      "Strategy                Score  Underpred      Risk Level\n",
      "-----------------------------------------------------\n",
      "conservative_60        0.7007         14        ðŸŸ¡ MEDIUM\n",
      "conservative_50        0.6861         24          ðŸ”´ HIGH\n",
      "raw                    0.7226         28          ðŸ”´ HIGH\n",
      "conservative_40        0.6934         28          ðŸ”´ HIGH\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATION\n",
      "======================================================================\n",
      "No strategy achieved sufficiently low underprediction risk.\n",
      "Consider using 'always_plus_2' for maximum safety.\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINED MODEL\n",
      "======================================================================\n",
      "Model trained on full dataset.\n",
      "Classes: [np.int64(1), np.int64(2), np.int64(4), np.int64(8), np.int64(16), np.int64(64)]\n",
      "\n",
      "Usage for prediction:\n",
      "  y_pred = conservative_predict(final_clf, X_new, threshold_classes, \n",
      "                                confidence_threshold=0.5, bump_steps=1)\n",
      "\n",
      "Or for maximum safety (minimize underprediction to near 0):\n",
      "  y_pred_safe = np.minimum(final_clf.predict(X_new) + 1, 5)  # Bump everything up\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL CONSOLIDATED MODEL - MINIMIZING UNDERPREDICTIONS\n",
    "# =============================================================================\n",
    "# Goal: Minimize underprediction risk (close to 0%) even if we overpredict more\n",
    "# Since we're tested on ONE scenario, a single underprediction = catastrophic 0 points\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD DATA AND FEATURE ENGINEERING\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"training_data_75.csv\")\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create domain-specific features for quantum circuit threshold prediction.\"\"\"\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Interaction features\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['degree_x_2q'] = X['avg_qubit_degree'] * X['n_2q_gates']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "    \n",
    "    # Ratio features\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['rotation_ratio'] = X['n_rotation_gates'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['edge_density'] = X['n_unique_edges'] / (X['n_qubits'] * (X['n_qubits'] - 1) / 2 + 1)\n",
    "    X['edge_repetition_ratio'] = X['n_edge_repetitions'] / (X['n_unique_edges'] + 1)\n",
    "    \n",
    "    # Polynomial features\n",
    "    X['degree_squared'] = X['avg_qubit_degree'] ** 2\n",
    "    X['qubits_squared'] = X['n_qubits'] ** 2\n",
    "    X['depth_squared'] = X['crude_depth'] ** 2\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    \n",
    "    # Complexity scores\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['entanglement_burden'] = X['n_2q_gates'] * X['avg_qubit_degree'] / (X['n_qubits'] + 1)\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    \n",
    "    # Pattern features\n",
    "    X['n_patterns'] = (X['has_qft_pattern'] + X['has_iqft_pattern'] + \n",
    "                       X['has_grover_pattern'] + X['has_variational_pattern'] + X['has_ghz_pattern'])\n",
    "    X['variational_complexity'] = X['has_variational_pattern'] * X['n_rotation_gates']\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features(df)\n",
    "\n",
    "# Drop non-feature columns\n",
    "drop_cols = [\"min_threshold\", \"file\", \"family\", \"forward_runtime\", \n",
    "             \"max_fidelity_achieved\", \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "\n",
    "# One-hot encode categoricals\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "y_raw = df[\"min_threshold\"].astype(int).values\n",
    "groups = df[\"file\"].astype(str).values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "threshold_classes = le.classes_  # [1, 2, 4, 8, 16, 64]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL MODEL: RandomForest + Top 10 Features + Conservative Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Threshold classes: {list(threshold_classes)}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. FEATURE SELECTION (Top 10 by CatBoost importance)\n",
    "# -----------------------------------------------------------------------------\n",
    "X_arr = X_eng.values.astype(np.float32)\n",
    "X_arr = np.nan_to_num(X_arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "clf_importance = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.05, random_seed=42, verbose=False)\n",
    "clf_importance.fit(X_arr, y)\n",
    "\n",
    "feature_importance = clf_importance.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_k = 10\n",
    "top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "X_top = X_eng[top_features].values.astype(np.float32)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Top {top_k} features (by CatBoost importance):\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. COMPETITION SCORING FUNCTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def competition_score(y_true, y_pred):\n",
    "    \"\"\"Calculate competition score (underpredict=0, exact=1, overpredict=partial)\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    scores = np.zeros(len(y_true))\n",
    "    scores[y_pred == y_true] = 1.0  # Exact match\n",
    "    over = y_pred > y_true\n",
    "    scores[over] = y_true[over] / y_pred[over]  # Partial credit\n",
    "    return scores\n",
    "\n",
    "def evaluate_strategy(y_true, y_pred, name=\"\"):\n",
    "    \"\"\"Evaluate with competition scoring\"\"\"\n",
    "    scores = competition_score(y_true, y_pred)\n",
    "    n = len(y_true)\n",
    "    exact = np.sum(y_pred == y_true)\n",
    "    under = np.sum(y_pred < y_true)\n",
    "    over = np.sum(y_pred > y_true)\n",
    "    \n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Score: {scores.sum():.1f}/{n} = {scores.mean():.4f} per sample\")\n",
    "    print(f\"  Exact: {exact:3d} ({100*exact/n:5.1f}%) â†’ {exact:.0f} pts\")\n",
    "    print(f\"  Under: {under:3d} ({100*under/n:5.1f}%) â†’ 0 pts [RISK!]\")\n",
    "    print(f\"  Over:  {over:3d} ({100*over/n:5.1f}%) â†’ {scores[y_pred > y_true].sum():.1f} pts\")\n",
    "    return scores.mean(), under\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. CONSERVATIVE PREDICTION FUNCTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def conservative_predict(clf, X, threshold_classes, confidence_threshold=0.6, bump_steps=1):\n",
    "    \"\"\"\n",
    "    Make conservative predictions that minimize underprediction risk.\n",
    "    \n",
    "    When model confidence is low, bump prediction up to higher threshold.\n",
    "    This trades off some exact matches for avoiding catastrophic underpredictions.\n",
    "    \"\"\"\n",
    "    proba = clf.predict_proba(X)\n",
    "    classes = clf.classes_\n",
    "    \n",
    "    predictions = []\n",
    "    for p in proba:\n",
    "        max_prob = p.max()\n",
    "        max_class_idx = p.argmax()\n",
    "        max_class = classes[max_class_idx]\n",
    "        \n",
    "        if max_prob < confidence_threshold:\n",
    "            # Not confident - bump up by bump_steps to be safe\n",
    "            new_idx = min(max_class_idx + bump_steps, len(classes) - 1)\n",
    "            predictions.append(classes[new_idx])\n",
    "        else:\n",
    "            predictions.append(max_class)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. K-FOLD EVALUATION WITH MULTIPLE STRATEGIES\n",
    "# -----------------------------------------------------------------------------\n",
    "min_class = min(np.bincount(y)[np.bincount(y) > 0])\n",
    "n_splits = min(3, min_class)\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Evaluation: {n_splits}-fold StratifiedGroupKFold (grouped by file)\")\n",
    "print()\n",
    "\n",
    "# Track results for different strategies\n",
    "strategies = {\n",
    "    'raw': {'true': [], 'pred': []},\n",
    "    'conservative_60': {'true': [], 'pred': []},\n",
    "    'conservative_50': {'true': [], 'pred': []},\n",
    "    'conservative_40': {'true': [], 'pred': []},\n",
    "}\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X_top, y, groups)):\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=10, min_samples_leaf=2,\n",
    "        class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_top[train_idx], y[train_idx])\n",
    "    \n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    # Raw prediction\n",
    "    y_raw_pred = clf.predict(X_top[test_idx])\n",
    "    strategies['raw']['true'].extend(y_test)\n",
    "    strategies['raw']['pred'].extend(y_raw_pred)\n",
    "    \n",
    "    # Conservative predictions with different confidence thresholds\n",
    "    for thresh, key in [(0.6, 'conservative_60'), (0.5, 'conservative_50'), (0.4, 'conservative_40')]:\n",
    "        y_cons = conservative_predict(clf, X_top[test_idx], threshold_classes, \n",
    "                                       confidence_threshold=thresh, bump_steps=1)\n",
    "        strategies[key]['true'].extend(y_test)\n",
    "        strategies[key]['pred'].extend(y_cons)\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGY COMPARISON (Minimizing Underprediction Risk)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "for name, data in strategies.items():\n",
    "    y_true_orig = le.inverse_transform(data['true'])\n",
    "    y_pred_orig = le.inverse_transform(data['pred'])\n",
    "    score, underpred = evaluate_strategy(y_true_orig, y_pred_orig, name)\n",
    "    results.append((name, score, underpred))\n",
    "    print()\n",
    "\n",
    "# Summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY - Sorted by Underprediction Risk (lowest first)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Strategy':<20} {'Score':>8} {'Underpred':>10} {'Risk Level':>15}\")\n",
    "print(\"-\" * 53)\n",
    "for name, score, underpred in sorted(results, key=lambda x: x[2]):\n",
    "    risk = \"ðŸŸ¢ LOW\" if underpred <= 5 else (\"ðŸŸ¡ MEDIUM\" if underpred <= 15 else \"ðŸ”´ HIGH\")\n",
    "    print(f\"{name:<20} {score:>8.4f} {underpred:>10d} {risk:>15}\")\n",
    "\n",
    "# Find best strategy with minimal underpredictions\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get strategy with lowest underprediction count that still has decent score\n",
    "safe_strategies = [(n, s, u) for n, s, u in results if u <= 10]\n",
    "if safe_strategies:\n",
    "    best = max(safe_strategies, key=lambda x: x[1])\n",
    "    print(f\"Best safe strategy: {best[0]}\")\n",
    "    print(f\"  Competition Score: {best[1]:.4f} per sample\")\n",
    "    print(f\"  Underpredictions: {best[2]} (goal: as close to 0 as possible)\")\n",
    "    print()\n",
    "    print(\"For a SINGLE test case, this minimizes the chance of getting 0 points.\")\n",
    "else:\n",
    "    print(\"No strategy achieved sufficiently low underprediction risk.\")\n",
    "    print(\"Consider using 'always_plus_2' for maximum safety.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. FINAL MODEL TRAINING (for production use)\n",
    "# -----------------------------------------------------------------------------\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TRAINED MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train on ALL data for final model\n",
    "final_clf = RandomForestClassifier(\n",
    "    n_estimators=500, max_depth=10, min_samples_leaf=2,\n",
    "    class_weight='balanced', random_state=42, n_jobs=-1\n",
    ")\n",
    "final_clf.fit(X_top, y)\n",
    "\n",
    "print(\"Model trained on full dataset.\")\n",
    "print(f\"Classes: {list(threshold_classes)}\")\n",
    "print()\n",
    "print(\"Usage for prediction:\")\n",
    "print(\"  y_pred = conservative_predict(final_clf, X_new, threshold_classes, \")\n",
    "print(\"                                confidence_threshold=0.5, bump_steps=1)\")\n",
    "print()\n",
    "print(\"Or for maximum safety (minimize underprediction to near 0):\")\n",
    "print(\"  y_pred_safe = np.minimum(final_clf.predict(X_new) + 1, 5)  # Bump everything up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e51074-b34c-475a-96f1-fccccdb2564f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
