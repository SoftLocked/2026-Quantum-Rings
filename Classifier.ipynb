{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-model-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THRESHOLD CLASSIFIER\n",
      "======================================================================\n",
      "\n",
      "Dataset: training_data.csv\n",
      "Total samples: 1107\n",
      "Unique files: 576\n",
      "\n",
      "Target: min_threshold\n",
      "    1: 434 samples ( 39.2%)\n",
      "    2: 336 samples ( 30.4%)\n",
      "    4: 114 samples ( 10.3%)\n",
      "    8:  80 samples (  7.2%)\n",
      "   16:  56 samples (  5.1%)\n",
      "   32:  58 samples (  5.2%)\n",
      "   64:  29 samples (  2.6%)\n",
      "\n",
      "Categorical columns: ['backend', 'precision']\n",
      "Feature matrix shape: (1107, 88)\n",
      "Threshold classes: [np.int64(1), np.int64(2), np.int64(4), np.int64(8), np.int64(16), np.int64(32), np.int64(64)]\n",
      "\n",
      "Using 5-fold StratifiedGroupKFold (grouped by file)\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating ExtraTrees...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating AdaBoost...\n",
      "Evaluating LogisticReg...\n",
      "\n",
      "======================================================================\n",
      "RESULTS - Sorted by Competition Score (higher is better)\n",
      "======================================================================\n",
      "\n",
      "Model                 CompScore   Accuracy   Under%    Over%\n",
      "------------------------------------------------------------\n",
      "GradientBoosting         0.9219     0.9033     5.8     3.9\n",
      "XGBoost                  0.9168     0.8997     6.2     3.8\n",
      "LightGBM                 0.9134     0.8925     5.9     4.9\n",
      "ExtraTrees               0.9134     0.8925     6.0     4.8\n",
      "RandomForest             0.9119     0.8979     6.8     3.4\n",
      "LogisticReg              0.8985     0.8726     6.0     6.8\n",
      "AdaBoost                 0.6825     0.6676    30.0     3.3\n",
      "\n",
      "Best Model: GradientBoosting\n",
      "  Competition Score: 0.9219\n",
      "  Accuracy: 0.9033\n",
      "  Under-rate: 5.8%\n",
      "  Over-rate: 3.9%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# THRESHOLD CLASSIFIER - Model Comparison\n",
    "# =============================================================================\n",
    "# Goal: Predict min_threshold class given circuit features\n",
    "# Dataset: training_data.csv (0.99 fidelity threshold, CPU-only)\n",
    "# Scoring: competition score (exact=1, over=true/pred, under=0)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
    "    AdaBoostClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"THRESHOLD CLASSIFIER\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Dataset: training_data.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique files: {df['file'].nunique()}\")\n",
    "print()\n",
    "\n",
    "print(\"Target: min_threshold\")\n",
    "threshold_dist = df['min_threshold'].value_counts().sort_index()\n",
    "for threshold, count in threshold_dist.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"  {threshold:>3d}: {count:3d} samples ({pct:5.1f}%)\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING\n",
    "# -----------------------------------------------------------------------------\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create domain-specific features for threshold prediction.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # Interaction features\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['degree_x_2q'] = X['avg_qubit_degree'] * X['n_2q_gates']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "\n",
    "    # Ratio features\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['rotation_ratio'] = X['n_rotation_gates'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['edge_density'] = X['n_unique_edges'] / (X['n_qubits'] * (X['n_qubits'] - 1) / 2 + 1)\n",
    "    X['edge_repetition_ratio'] = X['n_edge_repetitions'] / (X['n_unique_edges'] + 1)\n",
    "\n",
    "    # Polynomial / log features\n",
    "    X['degree_squared'] = X['avg_qubit_degree'] ** 2\n",
    "    X['qubits_squared'] = X['n_qubits'] ** 2\n",
    "    X['depth_squared'] = X['crude_depth'] ** 2\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "\n",
    "    # Complexity scores\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['entanglement_burden'] = X['n_2q_gates'] * X['avg_qubit_degree'] / (X['n_qubits'] + 1)\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "\n",
    "    # Pattern features\n",
    "    X['n_patterns'] = (X['has_qft_pattern'] + X['has_iqft_pattern'] +\n",
    "                       X['has_grover_pattern'] + X['has_variational_pattern'] + X['has_ghz_pattern'])\n",
    "    X['variational_complexity'] = X['has_variational_pattern'] * X['n_rotation_gates']\n",
    "\n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features(df)\n",
    "\n",
    "# Target and groups\n",
    "y_raw = df['min_threshold'].astype(int).values\n",
    "groups = df['file'].astype(str).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "THRESHOLD_CLASSES = le.classes_\n",
    "\n",
    "# Drop non-feature columns\n",
    "drop_cols = [\"min_threshold\", \"file\", \"forward_runtime\",\n",
    "             \"max_fidelity_achieved\", \"n_thresholds_tested\", \"threshold_runtime\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "\n",
    "# One-hot encode categoricals\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "X = X_eng.values.astype(np.float32)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Threshold classes: {list(THRESHOLD_CLASSES)}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. SCORING FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def competition_score(y_true, y_pred):\n",
    "    \"\"\"Competition score: exact=1, over=true/pred, under=0.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    scores = np.zeros(len(y_true))\n",
    "    scores[y_pred == y_true] = 1.0\n",
    "    over = y_pred > y_true\n",
    "    scores[over] = y_true[over] / y_pred[over]\n",
    "    return scores\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. MODEL COMPARISON\n",
    "# -----------------------------------------------------------------------------\n",
    "min_class_count = min(np.bincount(y)[np.bincount(y) > 0])\n",
    "n_splits = min(5, min_class_count)\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Using {n_splits}-fold StratifiedGroupKFold (grouped by file)\")\n",
    "print()\n",
    "\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbosity=0, use_label_encoder=False, eval_metric='mlogloss'\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbose=-1, class_weight='balanced'\n",
    "    ),\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'ExtraTrees': ExtraTreesClassifier(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=200, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "    'LogisticReg': LogisticRegression(\n",
    "        max_iter=1000, class_weight='balanced', random_state=42, n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "\n",
    "    y_pred_all = np.full(len(y), -1)\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups)):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X[train_idx])\n",
    "        X_test = scaler.transform(X[test_idx])\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y[train_idx])\n",
    "        y_pred_all[test_idx] = model_fold.predict(X_test)\n",
    "\n",
    "    # Convert back to original threshold values\n",
    "    y_true_orig = le.inverse_transform(y)\n",
    "    y_pred_orig = le.inverse_transform(y_pred_all)\n",
    "\n",
    "    scores = competition_score(y_true_orig, y_pred_orig)\n",
    "    acc = accuracy_score(y, y_pred_all)\n",
    "    under = np.mean(y_pred_orig < y_true_orig)\n",
    "    over = np.mean(y_pred_orig > y_true_orig)\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'comp_score': scores.mean(),\n",
    "        'accuracy': acc,\n",
    "        'under_rate': under,\n",
    "        'over_rate': over,\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS - Sorted by Competition Score (higher is better)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Model':<20} {'CompScore':>10} {'Accuracy':>10} {'Under%':>8} {'Over%':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in sorted(results, key=lambda x: -x['comp_score']):\n",
    "    print(f\"{r['model']:<20} {r['comp_score']:>10.4f} {r['accuracy']:>10.4f} \"\n",
    "          f\"{r['under_rate']*100:>7.1f} {r['over_rate']*100:>7.1f}\")\n",
    "\n",
    "print()\n",
    "best = max(results, key=lambda x: x['comp_score'])\n",
    "print(f\"Best Model: {best['model']}\")\n",
    "print(f\"  Competition Score: {best['comp_score']:.4f}\")\n",
    "print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "print(f\"  Under-rate: {best['under_rate']*100:.1f}%\")\n",
    "print(f\"  Over-rate: {best['over_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-2-feature-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SELECTION\n",
      "======================================================================\n",
      "\n",
      "Training RandomForest to get feature importances...\n",
      "\n",
      "Top 20 most important features:\n",
      "--------------------------------------------------\n",
      "  entanglement_per_qubit              0.0428\n",
      "  avg_gate_span                       0.0384\n",
      "  degree_squared                      0.0372\n",
      "  avg_qubit_degree                    0.0360\n",
      "  degree_x_qubits                     0.0317\n",
      "  entanglement_complexity             0.0312\n",
      "  std_gate_span                       0.0286\n",
      "  n_unique_edges                      0.0275\n",
      "  gates_per_layer_estimate            0.0257\n",
      "  max_gate_span                       0.0244\n",
      "  midpoint_cut_crossings              0.0237\n",
      "  max_qubit_degree                    0.0223\n",
      "  n_connected_components              0.0217\n",
      "  multi_qubit_ratio                   0.0182\n",
      "  degree_x_2q                         0.0181\n",
      "  depth_squared                       0.0179\n",
      "  degree_x_depth                      0.0177\n",
      "  depth_per_qubit                     0.0176\n",
      "  edge_density                        0.0174\n",
      "  ratio_1q_gates                      0.0167\n",
      "\n",
      "Optimal number of features (using best model from above):\n",
      "--------------------------------------------------\n",
      "  Top 10 features: CompScore = 0.8527, Accuracy = 0.8293\n",
      "  Top 20 features: CompScore = 0.8931, Accuracy = 0.8735\n",
      "  Top 30 features: CompScore = 0.8994, Accuracy = 0.8790\n",
      "  Top 40 features: CompScore = 0.9130, Accuracy = 0.8943\n",
      "  Top 50 features: CompScore = 0.9106, Accuracy = 0.8907\n",
      "  Top 60 features: CompScore = 0.9179, Accuracy = 0.9006\n",
      "  Top 70 features: CompScore = 0.9181, Accuracy = 0.8997\n",
      "  Top 80 features: CompScore = 0.9199, Accuracy = 0.9024\n",
      "  Top 88 features: CompScore = 0.9223, Accuracy = 0.9061\n",
      "\n",
      "Best feature count: 88 (CompScore = 0.9223)\n",
      "\n",
      "Selected Top 88 Features:\n",
      "   1. entanglement_per_qubit              (0.0428)\n",
      "   2. avg_gate_span                       (0.0384)\n",
      "   3. degree_squared                      (0.0372)\n",
      "   4. avg_qubit_degree                    (0.0360)\n",
      "   5. degree_x_qubits                     (0.0317)\n",
      "   6. entanglement_complexity             (0.0312)\n",
      "   7. std_gate_span                       (0.0286)\n",
      "   8. n_unique_edges                      (0.0275)\n",
      "   9. gates_per_layer_estimate            (0.0257)\n",
      "  10. max_gate_span                       (0.0244)\n",
      "  11. midpoint_cut_crossings              (0.0237)\n",
      "  12. max_qubit_degree                    (0.0223)\n",
      "  13. n_connected_components              (0.0217)\n",
      "  14. multi_qubit_ratio                   (0.0182)\n",
      "  15. degree_x_2q                         (0.0181)\n",
      "  16. depth_squared                       (0.0179)\n",
      "  17. degree_x_depth                      (0.0177)\n",
      "  18. depth_per_qubit                     (0.0176)\n",
      "  19. edge_density                        (0.0174)\n",
      "  20. ratio_1q_gates                      (0.0167)\n",
      "  21. cx_ratio                            (0.0164)\n",
      "  22. log_depth                           (0.0163)\n",
      "  23. n_cx                                (0.0159)\n",
      "  24. 1q_gates_per_qubit                  (0.0157)\n",
      "  25. crude_depth                         (0.0156)\n",
      "  26. entanglement_burden                 (0.0154)\n",
      "  27. edge_repetition_ratio               (0.0147)\n",
      "  28. complexity_score                    (0.0146)\n",
      "  29. circuit_density                     (0.0143)\n",
      "  30. ratio_2q_gates                      (0.0143)\n",
      "  31. qubit_degree_std                    (0.0135)\n",
      "  32. n_u2                                (0.0127)\n",
      "  33. n_edge_repetitions                  (0.0123)\n",
      "  34. entanglement_pressure               (0.0123)\n",
      "  35. n_h                                 (0.0121)\n",
      "  36. n_swap                              (0.0120)\n",
      "  37. gates_per_depth                     (0.0115)\n",
      "  38. precision_double                    (0.0112)\n",
      "  39. n_ccx                               (0.0111)\n",
      "  40. precision_single                    (0.0100)\n",
      "  41. n_lines                             (0.0099)\n",
      "  42. n_x                                 (0.0097)\n",
      "  43. n_3q_gates                          (0.0097)\n",
      "  44. 2q_gates_per_qubit                  (0.0095)\n",
      "  45. n_measure                           (0.0094)\n",
      "  46. n_cz                                (0.0094)\n",
      "  47. n_1q_gates                          (0.0092)\n",
      "  48. n_nonempty_lines                    (0.0091)\n",
      "  49. n_rotation_gates                    (0.0091)\n",
      "  50. n_total_gates                       (0.0087)\n",
      "  51. n_qubits                            (0.0084)\n",
      "  52. n_2q_gates                          (0.0083)\n",
      "  53. variational_complexity              (0.0082)\n",
      "  54. qubits_squared                      (0.0082)\n",
      "  55. n_classical_bits                    (0.0081)\n",
      "  56. log_gates                           (0.0081)\n",
      "  57. gates_per_qubit                     (0.0079)\n",
      "  58. log_qubits                          (0.0078)\n",
      "  59. n_u3                                (0.0076)\n",
      "  60. sim_difficulty                      (0.0070)\n",
      "  61. n_ry                                (0.0061)\n",
      "  62. rotation_ratio                      (0.0056)\n",
      "  63. has_grover_pattern                  (0.0052)\n",
      "  64. n_cp                                (0.0048)\n",
      "  65. n_qft_like_gates                    (0.0047)\n",
      "  66. n_u                                 (0.0030)\n",
      "  67. n_rx                                (0.0030)\n",
      "  68. n_barrier                           (0.0020)\n",
      "  69. n_patterns                          (0.0018)\n",
      "  70. n_qreg_declarations                 (0.0014)\n",
      "  71. has_qft_pattern                     (0.0012)\n",
      "  72. has_variational_pattern             (0.0009)\n",
      "  73. has_ghz_pattern                     (0.0002)\n",
      "  74. n_u1                                (0.0001)\n",
      "  75. n_s                                 (0.0000)\n",
      "  76. n_z                                 (0.0000)\n",
      "  77. n_y                                 (0.0000)\n",
      "  78. n_cy                                (0.0000)\n",
      "  79. n_ch                                (0.0000)\n",
      "  80. n_cswap                             (0.0000)\n",
      "  81. n_t                                 (0.0000)\n",
      "  82. n_sdg                               (0.0000)\n",
      "  83. n_opaque_gates                      (0.0000)\n",
      "  84. n_rz                                (0.0000)\n",
      "  85. n_tdg                               (0.0000)\n",
      "  86. n_custom_gates                      (0.0000)\n",
      "  87. has_iqft_pattern                    (0.0000)\n",
      "  88. backend_CPU                         (0.0000)\n",
      "\n",
      "======================================================================\n",
      "MODEL RE-COMPARISON WITH TOP 88 FEATURES\n",
      "======================================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating ExtraTrees...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating AdaBoost...\n",
      "Evaluating LogisticReg...\n",
      "\n",
      "Model                 CompScore   Accuracy   Under%    Over%\n",
      "------------------------------------------------------------\n",
      "GradientBoosting         0.9223     0.9061     6.1     3.3\n",
      "RandomForest             0.9171     0.9042     6.5     3.1\n",
      "LightGBM                 0.9142     0.8916     5.8     5.1\n",
      "XGBoost                  0.9137     0.8952     6.3     4.2\n",
      "ExtraTrees               0.9130     0.8943     6.1     4.5\n",
      "LogisticReg              0.8993     0.8735     6.0     6.7\n",
      "AdaBoost                 0.6825     0.6676    30.0     3.3\n",
      "\n",
      "Best model with top 88 features: GradientBoosting (CompScore = 0.9223)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION - Find optimal feature count\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Get feature importance using RandomForest\n",
    "print(\"Training RandomForest to get feature importances...\")\n",
    "scaler_init = StandardScaler()\n",
    "X_scaled_init = scaler_init.fit_transform(X)\n",
    "\n",
    "rf_imp = RandomForestClassifier(\n",
    "    n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "    class_weight='balanced', random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_imp.fit(X_scaled_init, y)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': rf_imp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print()\n",
    "print(\"Top 20 most important features:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in importance_df.head(20).iterrows():\n",
    "    print(f\"  {row['feature']:<35} {row['importance']:.4f}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Sweep feature counts with the best model from cell 1\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Optimal number of features (using best model from above):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_model_name = best['model']\n",
    "best_model_template = models[best_model_name]\n",
    "\n",
    "feature_sweep_results = []\n",
    "\n",
    "for top_k in list(range(10, len(importance_df), 10)) + [len(importance_df)]:\n",
    "    if top_k > len(importance_df):\n",
    "        continue\n",
    "\n",
    "    top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "    X_top = X_eng[top_features].values.astype(np.float32)\n",
    "    X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    y_pred_all = np.full(len(y), -1)\n",
    "\n",
    "    for train_idx, test_idx in sgkf.split(X_top, y, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = best_model_template.__class__(**best_model_template.get_params())\n",
    "        model_fold.fit(X_train, y[train_idx])\n",
    "        y_pred_all[test_idx] = model_fold.predict(X_test)\n",
    "\n",
    "    y_true_orig = le.inverse_transform(y)\n",
    "    y_pred_orig = le.inverse_transform(y_pred_all)\n",
    "    cs = competition_score(y_true_orig, y_pred_orig).mean()\n",
    "    acc = accuracy_score(y, y_pred_all)\n",
    "    feature_sweep_results.append({'k': top_k, 'comp_score': cs, 'accuracy': acc})\n",
    "    print(f\"  Top {top_k:2d} features: CompScore = {cs:.4f}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Pick best feature count\n",
    "best_k_result = max(feature_sweep_results, key=lambda x: x['comp_score'])\n",
    "BEST_K = best_k_result['k']\n",
    "TOP_FEATURES = importance_df.head(BEST_K)['feature'].tolist()\n",
    "\n",
    "print()\n",
    "print(f\"Best feature count: {BEST_K} (CompScore = {best_k_result['comp_score']:.4f})\")\n",
    "print()\n",
    "print(f\"Selected Top {BEST_K} Features:\")\n",
    "for i, feat in enumerate(TOP_FEATURES, 1):\n",
    "    imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:<35} ({imp:.4f})\")\n",
    "print()\n",
    "\n",
    "# Re-compare all models with the selected features\n",
    "print(\"=\" * 70)\n",
    "print(f\"MODEL RE-COMPARISON WITH TOP {BEST_K} FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "X_top = X_eng[TOP_FEATURES].values.astype(np.float32)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "results_top = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    y_pred_all = np.full(len(y), -1)\n",
    "\n",
    "    for train_idx, test_idx in sgkf.split(X_top, y, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y[train_idx])\n",
    "        y_pred_all[test_idx] = model_fold.predict(X_test)\n",
    "\n",
    "    y_true_orig = le.inverse_transform(y)\n",
    "    y_pred_orig = le.inverse_transform(y_pred_all)\n",
    "    scores = competition_score(y_true_orig, y_pred_orig)\n",
    "    acc = accuracy_score(y, y_pred_all)\n",
    "    under = np.mean(y_pred_orig < y_true_orig)\n",
    "    over = np.mean(y_pred_orig > y_true_orig)\n",
    "\n",
    "    results_top.append({\n",
    "        'model': name,\n",
    "        'comp_score': scores.mean(),\n",
    "        'accuracy': acc,\n",
    "        'under_rate': under,\n",
    "        'over_rate': over,\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(f\"{'Model':<20} {'CompScore':>10} {'Accuracy':>10} {'Under%':>8} {'Over%':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in sorted(results_top, key=lambda x: -x['comp_score']):\n",
    "    print(f\"{r['model']:<20} {r['comp_score']:>10.4f} {r['accuracy']:>10.4f} \"\n",
    "          f\"{r['under_rate']*100:>7.1f} {r['over_rate']*100:>7.1f}\")\n",
    "\n",
    "best_top = max(results_top, key=lambda x: x['comp_score'])\n",
    "BEST_MODEL_NAME = best_top['model']\n",
    "print()\n",
    "print(f\"Best model with top {BEST_K} features: {BEST_MODEL_NAME} (CompScore = {best_top['comp_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-3-hyperparam-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING: GradientBoosting with Top 88 Features\n",
      "======================================================================\n",
      "\n",
      "Running 40 Optuna trials for GradientBoosting...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 32. Best value: 0.925023: 100%|██████████| 40/40 [2:23:22<00:00, 215.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TUNING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Best Competition Score: 0.9250\n",
      "Best Accuracy: 0.9097\n",
      "Best Under-rate: 5.9%\n",
      "\n",
      "Best Hyperparameters:\n",
      "  n_estimators: 254\n",
      "  max_depth: 8\n",
      "  learning_rate: 0.020167\n",
      "  min_samples_split: 8\n",
      "  min_samples_leaf: 6\n",
      "  subsample: 0.947884\n",
      "\n",
      "======================================================================\n",
      "FINAL CROSS-VALIDATED METRICS\n",
      "======================================================================\n",
      "\n",
      "  Competition Score: 0.9250\n",
      "  Accuracy:          0.9097\n",
      "  Under-rate:        5.9%\n",
      "  Over-rate:         3.2%\n",
      "\n",
      "Baseline (all features, default params): CompScore = 0.9219\n",
      "After feature selection + tuning:         CompScore = 0.9250\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER TUNING - Optuna on best model\n",
    "# =============================================================================\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"HYPERPARAMETER TUNING: {BEST_MODEL_NAME} with Top {BEST_K} Features\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "X_top = X_eng[TOP_FEATURES].values.astype(np.float32)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def make_objective(model_name):\n",
    "    def objective(trial):\n",
    "        if model_name == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n",
    "                'random_state': 42,\n",
    "                'verbosity': 0,\n",
    "                'use_label_encoder': False,\n",
    "                'eval_metric': 'mlogloss'\n",
    "            }\n",
    "            model_class = XGBClassifier\n",
    "        elif model_name == 'LightGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'class_weight': 'balanced',\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model_class = LGBMClassifier\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model_class = GradientBoostingClassifier\n",
    "        elif model_name == 'RandomForest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']),\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model_class = RandomForestClassifier\n",
    "        elif model_name == 'ExtraTrees':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "                'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample']),\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model_class = ExtraTreesClassifier\n",
    "        else:\n",
    "            raise ValueError(f\"No tuning defined for {model_name}\")\n",
    "\n",
    "        y_pred_all = np.full(len(y), -1)\n",
    "\n",
    "        for train_idx, test_idx in sgkf.split(X_top, y, groups):\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_top[train_idx])\n",
    "            X_test = scaler.transform(X_top[test_idx])\n",
    "            X_train = np.clip(X_train, -10, 10)\n",
    "            X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "            model = model_class(**params)\n",
    "            model.fit(X_train, y[train_idx])\n",
    "            y_pred_all[test_idx] = model.predict(X_test)\n",
    "\n",
    "        y_true_orig = le.inverse_transform(y)\n",
    "        y_pred_orig = le.inverse_transform(y_pred_all)\n",
    "        cs = competition_score(y_true_orig, y_pred_orig).mean()\n",
    "        acc = accuracy_score(y, y_pred_all)\n",
    "        under = np.mean(y_pred_orig < y_true_orig)\n",
    "\n",
    "        trial.set_user_attr('accuracy', acc)\n",
    "        trial.set_user_attr('under_rate', under)\n",
    "        return cs\n",
    "\n",
    "    return objective\n",
    "\n",
    "# Run Optuna\n",
    "N_TRIALS = 40\n",
    "print(f\"Running {N_TRIALS} Optuna trials for {BEST_MODEL_NAME}...\")\n",
    "print()\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(make_objective(BEST_MODEL_NAME), n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Best Competition Score: {study.best_value:.4f}\")\n",
    "print(f\"Best Accuracy: {study.best_trial.user_attrs['accuracy']:.4f}\")\n",
    "print(f\"Best Under-rate: {study.best_trial.user_attrs['under_rate']*100:.1f}%\")\n",
    "print()\n",
    "print(\"Best Hyperparameters:\")\n",
    "BEST_PARAMS = study.best_params.copy()\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# Final evaluation with best params\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL CROSS-VALIDATED METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Add fixed params back\n",
    "if BEST_MODEL_NAME == 'XGBoost':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['verbosity'] = 0\n",
    "    BEST_PARAMS['use_label_encoder'] = False\n",
    "    BEST_PARAMS['eval_metric'] = 'mlogloss'\n",
    "    best_model_class = XGBClassifier\n",
    "elif BEST_MODEL_NAME == 'LightGBM':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['verbose'] = -1\n",
    "    BEST_PARAMS['class_weight'] = 'balanced'\n",
    "    best_model_class = LGBMClassifier\n",
    "elif BEST_MODEL_NAME == 'GradientBoosting':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    best_model_class = GradientBoostingClassifier\n",
    "elif BEST_MODEL_NAME == 'RandomForest':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['n_jobs'] = -1\n",
    "    best_model_class = RandomForestClassifier\n",
    "elif BEST_MODEL_NAME == 'ExtraTrees':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['n_jobs'] = -1\n",
    "    best_model_class = ExtraTreesClassifier\n",
    "\n",
    "y_pred_final = np.full(len(y), -1)\n",
    "\n",
    "for train_idx, test_idx in sgkf.split(X_top, y, groups):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_top[train_idx])\n",
    "    X_test = scaler.transform(X_top[test_idx])\n",
    "    X_train = np.clip(X_train, -10, 10)\n",
    "    X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "    model = best_model_class(**BEST_PARAMS)\n",
    "    model.fit(X_train, y[train_idx])\n",
    "    y_pred_final[test_idx] = model.predict(X_test)\n",
    "\n",
    "y_true_orig = le.inverse_transform(y)\n",
    "y_pred_orig = le.inverse_transform(y_pred_final)\n",
    "final_cs = competition_score(y_true_orig, y_pred_orig).mean()\n",
    "final_acc = accuracy_score(y, y_pred_final)\n",
    "final_under = np.mean(y_pred_orig < y_true_orig)\n",
    "final_over = np.mean(y_pred_orig > y_true_orig)\n",
    "\n",
    "print(f\"  Competition Score: {final_cs:.4f}\")\n",
    "print(f\"  Accuracy:          {final_acc:.4f}\")\n",
    "print(f\"  Under-rate:        {final_under*100:.1f}%\")\n",
    "print(f\"  Over-rate:         {final_over*100:.1f}%\")\n",
    "print()\n",
    "print(f\"Baseline (all features, default params): CompScore = {best['comp_score']:.4f}\")\n",
    "print(f\"After feature selection + tuning:         CompScore = {final_cs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-4-production-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRODUCTION THRESHOLD CLASSIFIER\n",
      "======================================================================\n",
      "\n",
      "Model: GradientBoosting\n",
      "Features: Top 88\n",
      "\n",
      "Hyperparameters:\n",
      "  n_estimators: 254\n",
      "  max_depth: 8\n",
      "  learning_rate: 0.020167\n",
      "  min_samples_split: 8\n",
      "  min_samples_leaf: 6\n",
      "  subsample: 0.947884\n",
      "  random_state: 42\n",
      "\n",
      "Model trained on full dataset (1107 samples).\n",
      "Classes: [np.int64(1), np.int64(2), np.int64(4), np.int64(8), np.int64(16), np.int64(32), np.int64(64)]\n",
      "\n",
      "Model saved to models\\threshold_classifier.pkl\n",
      "  File size: 9731.8 KB\n",
      "\n",
      "======================================================================\n",
      "SANITY CHECK ON TRAINING FILES\n",
      "======================================================================\n",
      "\n",
      "File                                          Prec     True  Pred   Conf Result  \n",
      "--------------------------------------------------------------------------------\n",
      "qftentangled_indep_qiskit_17.qasm             single      4     4   0.93 EXACT   \n",
      "ae_indep_qiskit_36.qasm                       double      4     4   1.00 EXACT   \n",
      "grover-noancilla_indep_qiskit_3.qasm          single      2     2   1.00 EXACT   \n",
      "wstate_indep_qiskit_43.qasm                   double      2     2   1.00 EXACT   \n",
      "qnn_indep_qiskit_6.qasm                       double      4     4   0.99 EXACT   \n",
      "qftentangled_indep_qiskit_20.qasm             double      2     2   1.00 EXACT   \n",
      "qft_indep_qiskit_28.qasm                      double      1     1   1.00 EXACT   \n",
      "ae_indep_qiskit_3.qasm                        single      2     2   1.00 EXACT   \n",
      "grover-noancilla_indep_qiskit_5.qasm          single      4     4   1.00 EXACT   \n",
      "portfolioqaoa_indep_qiskit_4.qasm             double      1     1   0.99 EXACT   \n",
      "ghz_indep_qiskit_10.qasm                      single      2     2   1.00 EXACT   \n",
      "qftentangled_indep_qiskit_12.qasm             single      2     2   1.00 EXACT   \n",
      "qwalk-v-chain_indep_qiskit_5.qasm             single      4     4   0.99 EXACT   \n",
      "ghz_indep_qiskit_40.qasm                      double      2     2   1.00 EXACT   \n",
      "graphstate_indep_qiskit_38.qasm               single     32    32   0.99 EXACT   \n",
      "\n",
      "Score: 15.0/15 = 1.0000\n",
      "\n",
      "Usage:\n",
      "  result = predict_threshold(\"circuits_new/your_circuit.qasm\", \"single\")\n",
      "  result = predict_threshold(\"circuits_new/your_circuit.qasm\", \"double\", conservative=True)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION THRESHOLD CLASSIFIER\n",
    "# =============================================================================\n",
    "# Trains on ALL data with tuned hyperparameters, exposes predict_threshold()\n",
    "# =============================================================================\n",
    "\n",
    "import joblib\n",
    "from comprehensive_features import QASMFeatureExtractor\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION THRESHOLD CLASSIFIER\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Model: {BEST_MODEL_NAME}\")\n",
    "print(f\"Features: Top {BEST_K}\")\n",
    "print()\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# Train on ALL data\n",
    "prod_scaler = StandardScaler()\n",
    "X_prod = prod_scaler.fit_transform(X_top)\n",
    "X_prod = np.clip(X_prod, -10, 10)\n",
    "\n",
    "prod_model = best_model_class(**BEST_PARAMS)\n",
    "prod_model.fit(X_prod, y)\n",
    "print(f\"Model trained on full dataset ({len(y)} samples).\")\n",
    "print(f\"Classes: {list(THRESHOLD_CLASSES)}\")\n",
    "print()\n",
    "\n",
    "# Save references for prediction\n",
    "PRODUCTION_FEATURES = TOP_FEATURES\n",
    "PRODUCTION_DROP_COLS = drop_cols\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# SAVE MODEL TO DISK\n",
    "# -------------------------------------------------------------------------\n",
    "model_artifact = {\n",
    "    'model': prod_model,\n",
    "    'scaler': prod_scaler,\n",
    "    'label_encoder': le,\n",
    "    'threshold_classes': THRESHOLD_CLASSES,\n",
    "    'features': PRODUCTION_FEATURES,\n",
    "    'drop_cols': PRODUCTION_DROP_COLS,\n",
    "    'model_name': BEST_MODEL_NAME,\n",
    "    'best_params': BEST_PARAMS,\n",
    "    'best_k': BEST_K,\n",
    "}\n",
    "\n",
    "save_path = Path(\"models/threshold_classifier.pkl\")\n",
    "joblib.dump(model_artifact, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(f\"  File size: {save_path.stat().st_size / 1024:.1f} KB\")\n",
    "print()\n",
    "\n",
    "\n",
    "def predict_threshold(file_path, precision, conservative=False, confidence_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict the optimal threshold for a QASM circuit.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the QASM file\n",
    "        precision: 'single' or 'double'\n",
    "        conservative: If True, bump up prediction when confidence is low\n",
    "        confidence_threshold: Confidence threshold for bumping (default 0.6)\n",
    "\n",
    "    Returns:\n",
    "        dict with prediction, confidence, and probabilities\n",
    "    \"\"\"\n",
    "    features = QASMFeatureExtractor(file_path).extract_all()\n",
    "    features['backend'] = 'CPU'\n",
    "    features['precision'] = precision\n",
    "\n",
    "    input_df = pd.DataFrame([features])\n",
    "    input_eng = engineer_features(input_df)\n",
    "\n",
    "    # Drop non-feature cols\n",
    "    for col in PRODUCTION_DROP_COLS:\n",
    "        if col in input_eng.columns:\n",
    "            input_eng = input_eng.drop(columns=[col])\n",
    "\n",
    "    # One-hot encode\n",
    "    cat = input_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    input_eng = pd.get_dummies(input_eng, columns=cat)\n",
    "\n",
    "    # Align with training features\n",
    "    for col in PRODUCTION_FEATURES:\n",
    "        if col not in input_eng.columns:\n",
    "            input_eng[col] = 0\n",
    "\n",
    "    X_input = input_eng[PRODUCTION_FEATURES].values.astype(np.float32)\n",
    "    X_input = np.nan_to_num(X_input, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_input = np.clip(prod_scaler.transform(X_input), -10, 10)\n",
    "\n",
    "    # Get prediction and probabilities\n",
    "    pred_encoded = prod_model.predict(X_input)[0]\n",
    "    confidence = 0.0\n",
    "    prob_dict = {}\n",
    "\n",
    "    if hasattr(prod_model, 'predict_proba'):\n",
    "        proba = prod_model.predict_proba(X_input)[0]\n",
    "        confidence = float(proba.max())\n",
    "        for i, cls in enumerate(prod_model.classes_):\n",
    "            prob_dict[int(THRESHOLD_CLASSES[cls])] = float(proba[i])\n",
    "\n",
    "        # Conservative: bump up if not confident\n",
    "        if conservative and confidence < confidence_threshold:\n",
    "            new_idx = min(pred_encoded + 1, len(THRESHOLD_CLASSES) - 1)\n",
    "            pred_encoded = new_idx\n",
    "\n",
    "    pred_threshold = int(le.inverse_transform([pred_encoded])[0])\n",
    "\n",
    "    return {\n",
    "        'predicted_threshold': pred_threshold,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': prob_dict,\n",
    "        'conservative_mode': conservative\n",
    "    }\n",
    "\n",
    "# Sanity check on training files\n",
    "print(\"=\" * 70)\n",
    "print(\"SANITY CHECK ON TRAINING FILES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "circuits_dir = Path(\"circuits_new\")\n",
    "test_rows = df.sample(n=min(15, len(df)), random_state=42)\n",
    "\n",
    "print(f\"{'File':<45} {'Prec':<7} {'True':>5} {'Pred':>5} {'Conf':>6} {'Result':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "total_score = 0\n",
    "for _, row in test_rows.iterrows():\n",
    "    qasm_path = circuits_dir / row['file']\n",
    "    result = predict_threshold(qasm_path, row['precision'])\n",
    "    true_t = int(row['min_threshold'])\n",
    "    pred_t = result['predicted_threshold']\n",
    "    conf = result['confidence']\n",
    "\n",
    "    if pred_t == true_t:\n",
    "        status = \"EXACT\"\n",
    "        score = 1.0\n",
    "    elif pred_t > true_t:\n",
    "        status = \"OVER\"\n",
    "        score = true_t / pred_t\n",
    "    else:\n",
    "        status = \"UNDER\"\n",
    "        score = 0.0\n",
    "    total_score += score\n",
    "\n",
    "    print(f\"{row['file']:<45} {row['precision']:<7} {true_t:>5} {pred_t:>5} {conf:>6.2f} {status:<8}\")\n",
    "\n",
    "print()\n",
    "print(f\"Score: {total_score:.1f}/{len(test_rows)} = {total_score/len(test_rows):.4f}\")\n",
    "print()\n",
    "print(\"Usage:\")\n",
    "print('  result = predict_threshold(\"circuits_new/your_circuit.qasm\", \"single\")')\n",
    "print('  result = predict_threshold(\"circuits_new/your_circuit.qasm\", \"double\", conservative=True)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
