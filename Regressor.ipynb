{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-model-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNTIME REGRESSION MODEL\n",
      "======================================================================\n",
      "\n",
      "Dataset: training_data.csv\n",
      "Total samples: 1107\n",
      "Unique files: 576\n",
      "\n",
      "Target: forward_runtime\n",
      "  Min: 3192.51 seconds\n",
      "  Max: 9158881.81 seconds\n",
      "  Mean: 286512.97 seconds\n",
      "  Median: 74048.02 seconds\n",
      "  Std: 667834.21 seconds\n",
      "\n",
      "Runtime distribution:\n",
      "  25th percentile: 25379.24 seconds\n",
      "  50th percentile: 74048.02 seconds\n",
      "  75th percentile: 243357.39 seconds\n",
      "  90th percentile: 740783.87 seconds\n",
      "  95th percentile: 1331608.45 seconds\n",
      "  99th percentile: 2752661.68 seconds\n",
      "\n",
      "min_threshold included as feature: True\n",
      "Categorical columns: ['backend', 'precision']\n",
      "Feature matrix shape: (1107, 84)\n",
      "\n",
      "Using 5-fold GroupKFold (grouped by file)\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION (predicting log(runtime), then transforming back)\n",
      "======================================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating ExtraTrees...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating AdaBoost...\n",
      "Evaluating SVR_linear...\n",
      "\n",
      "======================================================================\n",
      "RESULTS - Sorted by MAPE (lower is better)\n",
      "======================================================================\n",
      "\n",
      "Model                   MAPE%  MedAPE%        MAE       RMSE       R²\n",
      "----------------------------------------------------------------------\n",
      "GradientBoosting         10.5      7.7   33571.81  120601.42   0.9674\n",
      "ExtraTrees               10.7      7.1   36192.62  159713.40   0.9428\n",
      "XGBoost                  11.1      7.9   39545.36  155623.04   0.9456\n",
      "LightGBM                 11.8      8.5   48813.48  238562.46   0.8723\n",
      "RandomForest             12.0      8.8   41846.14  209060.91   0.9019\n",
      "SVR_linear               33.8     25.6  146114.28  747482.37  -0.2539\n",
      "AdaBoost                 46.5     41.9  151578.50  457598.59   0.5301\n",
      "\n",
      "Best Model: GradientBoosting\n",
      "  MAPE: 10.5%\n",
      "  MedAPE: 7.7%\n",
      "  MAE: 33571.81 seconds\n",
      "  R²: 0.9674\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RUNTIME REGRESSION MODEL - Model Comparison\n",
    "# =============================================================================\n",
    "# Goal: Predict forward_runtime given circuit features + threshold\n",
    "# Dataset: training_data.csv (0.99 fidelity threshold, CPU-only)\n",
    "# Key: min_threshold is used as a FEATURE (not target)\n",
    "# Primary metric: MAPE (lower is better)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor,\n",
    "    AdaBoostRegressor, BaggingRegressor\n",
    ")\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNTIME REGRESSION MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Dataset: training_data.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique files: {df['file'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# Target is forward_runtime\n",
    "print(\"Target: forward_runtime\")\n",
    "print(f\"  Min: {df['forward_runtime'].min():.2f} seconds\")\n",
    "print(f\"  Max: {df['forward_runtime'].max():.2f} seconds\")\n",
    "print(f\"  Mean: {df['forward_runtime'].mean():.2f} seconds\")\n",
    "print(f\"  Median: {df['forward_runtime'].median():.2f} seconds\")\n",
    "print(f\"  Std: {df['forward_runtime'].std():.2f} seconds\")\n",
    "print()\n",
    "\n",
    "print(\"Runtime distribution:\")\n",
    "for p in [25, 50, 75, 90, 95, 99]:\n",
    "    val = np.percentile(df['forward_runtime'], p)\n",
    "    print(f\"  {p}th percentile: {val:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING\n",
    "# -----------------------------------------------------------------------------\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create domain-specific features for runtime prediction.\"\"\"\n",
    "    X = df.copy()\n",
    "\n",
    "    # Interaction features\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "\n",
    "    # Ratio features\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['edge_density'] = X['n_unique_edges'] / (X['n_qubits'] * (X['n_qubits'] - 1) / 2 + 1)\n",
    "    X['edge_repetition_ratio'] = X['n_edge_repetitions'] / (X['n_unique_edges'] + 1)\n",
    "\n",
    "    # Log features (helps with skewed distributions)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "\n",
    "    # Complexity scores\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "\n",
    "    # Threshold-based features\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    X['threshold_x_gates'] = X['min_threshold'] * X['n_total_gates']\n",
    "\n",
    "    return X\n",
    "\n",
    "# Apply feature engineering\n",
    "X_eng = engineer_features(df)\n",
    "\n",
    "# Target variable - use LOG transform for skewed runtime\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Groups for cross-validation\n",
    "groups = df['file'].values\n",
    "\n",
    "# Columns to drop (non-features)\n",
    "drop_cols = [\"forward_runtime\", \"file\",\n",
    "             \"max_fidelity_achieved\", \"n_thresholds_tested\", \"threshold_runtime\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "\n",
    "print(f\"min_threshold included as feature: {'min_threshold' in X_eng.columns}\")\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "# Prepare arrays\n",
    "X = X_eng.values.astype(np.float64)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. MODEL COMPARISON\n",
    "# -----------------------------------------------------------------------------\n",
    "n_splits = 5\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "print(f\"Using {n_splits}-fold GroupKFold (grouped by file)\")\n",
    "print()\n",
    "\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbose=-1\n",
    "    ),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'ExtraTrees': ExtraTreesRegressor(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'AdaBoost': AdaBoostRegressor(\n",
    "        n_estimators=200, learning_rate=0.05, random_state=42\n",
    "    ),\n",
    "    'SVR_linear': SVR(kernel='linear', C=1.0),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION (predicting log(runtime), then transforming back)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "\n",
    "    y_pred_all = np.zeros(len(y))\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X, y_log, groups)):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X[train_idx])\n",
    "        X_test = scaler.transform(X[test_idx])\n",
    "\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y_log[train_idx])\n",
    "\n",
    "        y_pred_log = model_fold.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred = np.maximum(y_pred, 0)\n",
    "\n",
    "        y_pred_all[test_idx] = y_pred\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred_all))\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    medape = np.median(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'MedAPE': medape\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS - Sorted by MAPE (lower is better)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Model':<20} {'MAPE%':>8} {'MedAPE%':>8} {'MAE':>10} {'RMSE':>10} {'R²':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in sorted(results, key=lambda x: x['MAPE']):\n",
    "    print(f\"{r['model']:<20} {r['MAPE']:>8.1f} {r['MedAPE']:>8.1f} {r['MAE']:>10.2f} \"\n",
    "          f\"{r['RMSE']:>10.2f} {r['R2']:>8.4f}\")\n",
    "\n",
    "print()\n",
    "best = min(results, key=lambda x: x['MAPE'])\n",
    "print(f\"Best Model: {best['model']}\")\n",
    "print(f\"  MAPE: {best['MAPE']:.1f}%\")\n",
    "print(f\"  MedAPE: {best['MedAPE']:.1f}%\")\n",
    "print(f\"  MAE: {best['MAE']:.2f} seconds\")\n",
    "print(f\"  R²: {best['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2-feature-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SELECTION\n",
      "======================================================================\n",
      "\n",
      "Training RandomForest to get feature importances...\n",
      "\n",
      "Top 20 most important features:\n",
      "--------------------------------------------------\n",
      "  max_gate_span                       0.5690\n",
      "  log_qubits                          0.0699\n",
      "  n_qubits                            0.0631\n",
      "  n_measure                           0.0463\n",
      "  circuit_density                     0.0367\n",
      "  precision_single                    0.0340\n",
      "  precision_double                    0.0295\n",
      "  threshold_x_qubits                  0.0218\n",
      "  n_u3                                0.0205\n",
      "  n_u                                 0.0193\n",
      "  degree_x_qubits                     0.0099\n",
      "  n_1q_gates                          0.0098\n",
      "  n_h                                 0.0064\n",
      "  avg_qubit_degree                    0.0042\n",
      "  1q_gates_per_qubit                  0.0037\n",
      "  cx_ratio                            0.0035\n",
      "  n_2q_gates                          0.0034\n",
      "  std_gate_span                       0.0031\n",
      "  gates_per_layer_estimate            0.0029\n",
      "  threshold_x_gates                   0.0027\n",
      "\n",
      "Optimal number of features (using best model from above):\n",
      "--------------------------------------------------\n",
      "  Top 10 features: MAPE = 22.4%, MAE = 90608.48s\n",
      "  Top 20 features: MAPE = 10.4%, MAE = 34493.18s\n",
      "  Top 30 features: MAPE = 10.5%, MAE = 33833.94s\n",
      "  Top 40 features: MAPE = 10.4%, MAE = 33533.16s\n",
      "  Top 50 features: MAPE = 10.4%, MAE = 33490.44s\n",
      "  Top 60 features: MAPE = 10.6%, MAE = 35163.63s\n",
      "  Top 70 features: MAPE = 10.5%, MAE = 34210.66s\n",
      "  Top 80 features: MAPE = 10.4%, MAE = 34777.14s\n",
      "  Top 84 features: MAPE = 10.4%, MAE = 33928.18s\n",
      "\n",
      "Best feature count: 50 (MAPE = 10.4%)\n",
      "\n",
      "Selected Top 50 Features:\n",
      "   1. max_gate_span                       (0.5690)\n",
      "   2. log_qubits                          (0.0699)\n",
      "   3. n_qubits                            (0.0631)\n",
      "   4. n_measure                           (0.0463)\n",
      "   5. circuit_density                     (0.0367)\n",
      "   6. precision_single                    (0.0340)\n",
      "   7. precision_double                    (0.0295)\n",
      "   8. threshold_x_qubits                  (0.0218)\n",
      "   9. n_u3                                (0.0205)\n",
      "  10. n_u                                 (0.0193)\n",
      "  11. degree_x_qubits                     (0.0099)\n",
      "  12. n_1q_gates                          (0.0098)\n",
      "  13. n_h                                 (0.0064)\n",
      "  14. avg_qubit_degree                    (0.0042)\n",
      "  15. 1q_gates_per_qubit                  (0.0037)\n",
      "  16. cx_ratio                            (0.0035)\n",
      "  17. n_2q_gates                          (0.0034)\n",
      "  18. std_gate_span                       (0.0031)\n",
      "  19. gates_per_layer_estimate            (0.0029)\n",
      "  20. threshold_x_gates                   (0.0027)\n",
      "  21. midpoint_cut_crossings              (0.0025)\n",
      "  22. edge_density                        (0.0022)\n",
      "  23. qubit_degree_std                    (0.0021)\n",
      "  24. depth_per_qubit                     (0.0019)\n",
      "  25. gates_per_depth                     (0.0018)\n",
      "  26. n_classical_bits                    (0.0017)\n",
      "  27. entanglement_per_qubit              (0.0017)\n",
      "  28. avg_gate_span                       (0.0016)\n",
      "  29. sim_difficulty                      (0.0014)\n",
      "  30. 2q_gates_per_qubit                  (0.0014)\n",
      "  31. entanglement_pressure               (0.0013)\n",
      "  32. entanglement_complexity             (0.0013)\n",
      "  33. log_threshold                       (0.0013)\n",
      "  34. complexity_score                    (0.0012)\n",
      "  35. min_threshold                       (0.0012)\n",
      "  36. n_cx                                (0.0011)\n",
      "  37. n_swap                              (0.0011)\n",
      "  38. n_lines                             (0.0011)\n",
      "  39. n_unique_edges                      (0.0010)\n",
      "  40. crude_depth                         (0.0010)\n",
      "  41. gates_per_qubit                     (0.0010)\n",
      "  42. degree_x_depth                      (0.0010)\n",
      "  43. n_u2                                (0.0008)\n",
      "  44. n_nonempty_lines                    (0.0007)\n",
      "  45. n_edge_repetitions                  (0.0007)\n",
      "  46. log_gates                           (0.0007)\n",
      "  47. log_depth                           (0.0007)\n",
      "  48. ratio_1q_gates                      (0.0007)\n",
      "  49. multi_qubit_ratio                   (0.0006)\n",
      "  50. ratio_2q_gates                      (0.0005)\n",
      "\n",
      "======================================================================\n",
      "MODEL RE-COMPARISON WITH TOP 50 FEATURES\n",
      "======================================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating ExtraTrees...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating AdaBoost...\n",
      "Evaluating SVR_linear...\n",
      "\n",
      "Model                   MAPE%  MedAPE%        MAE       RMSE       R²\n",
      "----------------------------------------------------------------------\n",
      "GradientBoosting         10.4      7.3   33490.44  123007.32   0.9660\n",
      "ExtraTrees               10.7      7.1   36451.98  159666.06   0.9428\n",
      "XGBoost                  11.2      8.0   40675.24  168585.86   0.9362\n",
      "LightGBM                 11.8      8.6   48876.41  235752.80   0.8753\n",
      "RandomForest             12.0      8.9   42086.18  210816.96   0.9003\n",
      "SVR_linear               33.6     24.9  155110.16  844599.28  -0.6009\n",
      "AdaBoost                 46.1     41.2  149739.41  455494.63   0.5344\n",
      "\n",
      "Best model with top 50 features: GradientBoosting (MAPE = 10.4%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION - Find optimal feature count\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Get feature importance using RandomForest\n",
    "print(\"Training RandomForest to get feature importances...\")\n",
    "scaler_init = StandardScaler()\n",
    "X_scaled_init = scaler_init.fit_transform(X)\n",
    "\n",
    "rf_imp = RandomForestRegressor(\n",
    "    n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_imp.fit(X_scaled_init, y_log)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': rf_imp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print()\n",
    "print(\"Top 20 most important features:\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in importance_df.head(20).iterrows():\n",
    "    print(f\"  {row['feature']:<35} {row['importance']:.4f}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Try different feature counts with the best model from cell 1\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Optimal number of features (using best model from above):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_model_name = best['model']\n",
    "best_model_template = models[best_model_name]\n",
    "\n",
    "feature_sweep_results = []\n",
    "\n",
    "for top_k in list(range(10, len(importance_df), 10)) + [len(importance_df)]:\n",
    "    if top_k > len(importance_df):\n",
    "        continue\n",
    "\n",
    "    top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "    X_top = X_eng[top_features].values.astype(np.float64)\n",
    "    X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    y_pred_all = np.zeros(len(y))\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = best_model_template.__class__(**best_model_template.get_params())\n",
    "        model_fold.fit(X_train, y_log[train_idx])\n",
    "        y_pred_log = model_fold.predict(X_test)\n",
    "        y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "\n",
    "    mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    feature_sweep_results.append({'k': top_k, 'MAPE': mape, 'MAE': mae})\n",
    "    print(f\"  Top {top_k:2d} features: MAPE = {mape:.1f}%, MAE = {mae:.2f}s\")\n",
    "\n",
    "# Pick best feature count (lowest MAPE)\n",
    "best_k_result = min(feature_sweep_results, key=lambda x: x['MAPE'])\n",
    "BEST_K = best_k_result['k']\n",
    "TOP_FEATURES = importance_df.head(BEST_K)['feature'].tolist()\n",
    "\n",
    "print()\n",
    "print(f\"Best feature count: {BEST_K} (MAPE = {best_k_result['MAPE']:.1f}%)\")\n",
    "print()\n",
    "print(f\"Selected Top {BEST_K} Features:\")\n",
    "for i, feat in enumerate(TOP_FEATURES, 1):\n",
    "    imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:<35} ({imp:.4f})\")\n",
    "print()\n",
    "\n",
    "# Also re-compare all models with the selected features\n",
    "print(\"=\" * 70)\n",
    "print(f\"MODEL RE-COMPARISON WITH TOP {BEST_K} FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "X_top = X_eng[TOP_FEATURES].values.astype(np.float64)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "results_top = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    y_pred_all = np.zeros(len(y))\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y_log[train_idx])\n",
    "        y_pred_log = model_fold.predict(X_test)\n",
    "        y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred_all))\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    medape = np.median(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "\n",
    "    results_top.append({\n",
    "        'model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'MedAPE': medape\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(f\"{'Model':<20} {'MAPE%':>8} {'MedAPE%':>8} {'MAE':>10} {'RMSE':>10} {'R²':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in sorted(results_top, key=lambda x: x['MAPE']):\n",
    "    print(f\"{r['model']:<20} {r['MAPE']:>8.1f} {r['MedAPE']:>8.1f} {r['MAE']:>10.2f} \"\n",
    "          f\"{r['RMSE']:>10.2f} {r['R2']:>8.4f}\")\n",
    "\n",
    "best_top = min(results_top, key=lambda x: x['MAPE'])\n",
    "BEST_MODEL_NAME = best_top['model']\n",
    "print()\n",
    "print(f\"Best model with top {BEST_K} features: {BEST_MODEL_NAME} (MAPE = {best_top['MAPE']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3-hyperparam-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING: GradientBoosting with Top 50 Features\n",
      "======================================================================\n",
      "\n",
      "Running 200 Optuna trials for GradientBoosting...\n",
      "Objective: minimize MAPE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 179. Best value: 9.55613: 100%|██████████| 200/200 [53:58<00:00, 16.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TUNING RESULTS\n",
      "======================================================================\n",
      "\n",
      "Best MAPE: 9.6%\n",
      "Best R²:   0.9666\n",
      "Best MAE:  31260.56s\n",
      "\n",
      "Best Hyperparameters:\n",
      "  n_estimators: 794\n",
      "  max_depth: 5\n",
      "  learning_rate: 0.011740\n",
      "  min_samples_split: 3\n",
      "  min_samples_leaf: 1\n",
      "  subsample: 0.572111\n",
      "\n",
      "======================================================================\n",
      "FINAL CROSS-VALIDATED METRICS\n",
      "======================================================================\n",
      "\n",
      "  MAPE:   9.6%\n",
      "  MedAPE: 6.9%\n",
      "  MAE:    31260.56s\n",
      "  RMSE:   121989.43s\n",
      "  R²:     0.9666\n",
      "\n",
      "Baseline (all features, default params): MAPE = 10.5%\n",
      "After feature selection + tuning:         MAPE = 9.6%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER TUNING - Optuna on best model (minimizing MAPE)\n",
    "# =============================================================================\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"HYPERPARAMETER TUNING: {BEST_MODEL_NAME} with Top {BEST_K} Features\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Prepare feature matrix\n",
    "X_top = X_eng[TOP_FEATURES].values.astype(np.float64)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Define objective functions for each possible best model\n",
    "def make_objective(model_name):\n",
    "    def objective(trial):\n",
    "        if model_name == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n",
    "                'random_state': 42,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            model_class = XGBRegressor\n",
    "        elif model_name == 'LightGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model_class = LGBMRegressor\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5, log=True),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 15),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model_class = GradientBoostingRegressor\n",
    "        elif model_name == 'RandomForest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model_class = RandomForestRegressor\n",
    "        elif model_name == 'ExtraTrees':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model_class = ExtraTreesRegressor\n",
    "        else:\n",
    "            raise ValueError(f\"No tuning defined for {model_name}\")\n",
    "\n",
    "        y_pred_all = np.zeros(len(y))\n",
    "\n",
    "        for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_top[train_idx])\n",
    "            X_test = scaler.transform(X_top[test_idx])\n",
    "            X_train = np.clip(X_train, -10, 10)\n",
    "            X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "            model = model_class(**params)\n",
    "            model.fit(X_train, y_log[train_idx])\n",
    "            y_pred_log = model.predict(X_test)\n",
    "            y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "\n",
    "        mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "        r2 = r2_score(y, y_pred_all)\n",
    "        mae = mean_absolute_error(y, y_pred_all)\n",
    "        trial.set_user_attr('r2', r2)\n",
    "        trial.set_user_attr('mae', mae)\n",
    "        return mape\n",
    "\n",
    "    return objective\n",
    "\n",
    "# Run Optuna (minimize MAPE)\n",
    "N_TRIALS = 200\n",
    "print(f\"Running {N_TRIALS} Optuna trials for {BEST_MODEL_NAME}...\")\n",
    "print(f\"Objective: minimize MAPE\")\n",
    "print()\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(make_objective(BEST_MODEL_NAME), n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Best MAPE: {study.best_value:.1f}%\")\n",
    "print(f\"Best R²:   {study.best_trial.user_attrs['r2']:.4f}\")\n",
    "print(f\"Best MAE:  {study.best_trial.user_attrs['mae']:.2f}s\")\n",
    "print()\n",
    "print(\"Best Hyperparameters:\")\n",
    "BEST_PARAMS = study.best_params.copy()\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# Final evaluation with best params\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL CROSS-VALIDATED METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Add fixed params back\n",
    "if BEST_MODEL_NAME == 'XGBoost':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['verbosity'] = 0\n",
    "    best_model_class = XGBRegressor\n",
    "elif BEST_MODEL_NAME == 'LightGBM':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['verbose'] = -1\n",
    "    best_model_class = LGBMRegressor\n",
    "elif BEST_MODEL_NAME == 'GradientBoosting':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    best_model_class = GradientBoostingRegressor\n",
    "elif BEST_MODEL_NAME == 'RandomForest':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['n_jobs'] = -1\n",
    "    best_model_class = RandomForestRegressor\n",
    "elif BEST_MODEL_NAME == 'ExtraTrees':\n",
    "    BEST_PARAMS['random_state'] = 42\n",
    "    BEST_PARAMS['n_jobs'] = -1\n",
    "    best_model_class = ExtraTreesRegressor\n",
    "\n",
    "y_pred_final = np.zeros(len(y))\n",
    "\n",
    "for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_top[train_idx])\n",
    "    X_test = scaler.transform(X_top[test_idx])\n",
    "    X_train = np.clip(X_train, -10, 10)\n",
    "    X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "    model = best_model_class(**BEST_PARAMS)\n",
    "    model.fit(X_train, y_log[train_idx])\n",
    "    y_pred_final[test_idx] = np.maximum(np.expm1(model.predict(X_test)), 0)\n",
    "\n",
    "final_mape = np.mean(np.abs(y - y_pred_final) / np.maximum(y, 1.0)) * 100\n",
    "final_medape = np.median(np.abs(y - y_pred_final) / np.maximum(y, 1.0)) * 100\n",
    "\n",
    "print(f\"  MAPE:   {final_mape:.1f}%\")\n",
    "print(f\"  MedAPE: {final_medape:.1f}%\")\n",
    "print(f\"  MAE:    {mean_absolute_error(y, y_pred_final):.2f}s\")\n",
    "print(f\"  RMSE:   {np.sqrt(mean_squared_error(y, y_pred_final)):.2f}s\")\n",
    "print(f\"  R²:     {r2_score(y, y_pred_final):.4f}\")\n",
    "print()\n",
    "print(f\"Baseline (all features, default params): MAPE = {best['MAPE']:.1f}%\")\n",
    "print(f\"After feature selection + tuning:         MAPE = {final_mape:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4-production-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRODUCTION RUNTIME PREDICTOR\n",
      "======================================================================\n",
      "\n",
      "Model: GradientBoosting\n",
      "Features: Top 50\n",
      "\n",
      "Hyperparameters:\n",
      "  n_estimators: 794\n",
      "  max_depth: 5\n",
      "  learning_rate: 0.011740\n",
      "  min_samples_split: 3\n",
      "  min_samples_leaf: 1\n",
      "  subsample: 0.572111\n",
      "  random_state: 42\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATED METRICS (5-fold GroupKFold)\n",
      "======================================================================\n",
      "\n",
      "  Fold 1: MAPE = 9.3%, MAE = 20623.99s, R² = 0.9882\n",
      "  Fold 2: MAPE = 9.2%, MAE = 25989.48s, R² = 0.9734\n",
      "  Fold 3: MAPE = 9.7%, MAE = 36050.26s, R² = 0.9420\n",
      "  Fold 4: MAPE = 9.9%, MAE = 34826.43s, R² = 0.9714\n",
      "  Fold 5: MAPE = 9.6%, MAE = 38884.61s, R² = 0.9663\n",
      "\n",
      "  Overall MAPE:   9.6%\n",
      "  Overall MedAPE: 6.9%\n",
      "  Overall MAE:    31260.56s\n",
      "  Overall RMSE:   121989.43s\n",
      "  Overall R²:     0.9666\n",
      "\n",
      "Production model trained on full dataset (1107 samples).\n",
      "\n",
      "Model saved to models\\runtime_model.pkl\n",
      "  File size: 2994.3 KB\n",
      "\n",
      "Verified model loads successfully.\n",
      "  Model type: GradientBoostingRegressor\n",
      "  Features: 50\n",
      "\n",
      "======================================================================\n",
      "SANITY CHECK ON TRAINING FILES\n",
      "======================================================================\n",
      "\n",
      "File                                          Prec    Thresh       True       Pred     Err%\n",
      "------------------------------------------------------------------------------------------\n",
      "qftentangled_indep_qiskit_17.qasm             single       4   60004.99   65718.18     9.5%\n",
      "ae_indep_qiskit_36.qasm                       double       4   80691.68   84190.94     4.3%\n",
      "grover-noancilla_indep_qiskit_3.qasm          single       2    5030.42    5098.56     1.4%\n",
      "wstate_indep_qiskit_43.qasm                   double       2  101917.73   99378.78     2.5%\n",
      "qnn_indep_qiskit_6.qasm                       double       4   19783.36   20654.73     4.4%\n",
      "qftentangled_indep_qiskit_20.qasm             double       2  204138.36  213071.64     4.4%\n",
      "qft_indep_qiskit_28.qasm                      double       1  410360.17  394715.97     3.8%\n",
      "ae_indep_qiskit_3.qasm                        single       2    5318.80    5244.06     1.4%\n",
      "grover-noancilla_indep_qiskit_5.qasm          single       4    9266.06    9766.81     5.4%\n",
      "portfolioqaoa_indep_qiskit_4.qasm             double       1    6913.29    6799.56     1.6%\n",
      "\n",
      "======================================================================\n",
      "USAGE - Loading saved model\n",
      "======================================================================\n",
      "\n",
      "  import joblib\n",
      "  artifact = joblib.load('models/runtime_model.pkl')\n",
      "  model = artifact['model']\n",
      "  scaler = artifact['scaler']\n",
      "  features = artifact['features']\n",
      "  cv = artifact['cv_metrics']  # MAPE, MedAPE, MAE, RMSE, R²\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION RUNTIME PREDICTOR\n",
    "# =============================================================================\n",
    "# Trains on ALL data with tuned hyperparameters, saves to models/\n",
    "# =============================================================================\n",
    "\n",
    "import joblib\n",
    "from comprehensive_features import QASMFeatureExtractor\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PRODUCTION RUNTIME PREDICTOR\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Model: {BEST_MODEL_NAME}\")\n",
    "print(f\"Features: Top {BEST_K}\")\n",
    "print()\n",
    "print(\"Hyperparameters:\")\n",
    "for k, v in BEST_PARAMS.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# CROSS-VALIDATED METRICS (before training on all data)\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-VALIDATED METRICS (5-fold GroupKFold)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "y_pred_cv = np.zeros(len(y))\n",
    "fold_metrics = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X_top, y_log, groups)):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_top[train_idx])\n",
    "    X_test = scaler.transform(X_top[test_idx])\n",
    "    X_train = np.clip(X_train, -10, 10)\n",
    "    X_test = np.clip(X_test, -10, 10)\n",
    "\n",
    "    cv_model = best_model_class(**BEST_PARAMS)\n",
    "    cv_model.fit(X_train, y_log[train_idx])\n",
    "    y_pred_cv[test_idx] = np.maximum(np.expm1(cv_model.predict(X_test)), 0)\n",
    "\n",
    "    # Per-fold metrics\n",
    "    fold_y = y[test_idx]\n",
    "    fold_pred = y_pred_cv[test_idx]\n",
    "    fold_mape = np.mean(np.abs(fold_y - fold_pred) / np.maximum(fold_y, 1.0)) * 100\n",
    "    fold_mae = mean_absolute_error(fold_y, fold_pred)\n",
    "    fold_r2 = r2_score(fold_y, fold_pred)\n",
    "    fold_metrics.append({'fold': fold_idx + 1, 'MAPE': fold_mape, 'MAE': fold_mae, 'R2': fold_r2})\n",
    "    print(f\"  Fold {fold_idx + 1}: MAPE = {fold_mape:.1f}%, MAE = {fold_mae:.2f}s, R² = {fold_r2:.4f}\")\n",
    "\n",
    "cv_mape = np.mean(np.abs(y - y_pred_cv) / np.maximum(y, 1.0)) * 100\n",
    "cv_medape = np.median(np.abs(y - y_pred_cv) / np.maximum(y, 1.0)) * 100\n",
    "cv_mae = mean_absolute_error(y, y_pred_cv)\n",
    "cv_rmse = np.sqrt(mean_squared_error(y, y_pred_cv))\n",
    "cv_r2 = r2_score(y, y_pred_cv)\n",
    "\n",
    "print()\n",
    "print(f\"  Overall MAPE:   {cv_mape:.1f}%\")\n",
    "print(f\"  Overall MedAPE: {cv_medape:.1f}%\")\n",
    "print(f\"  Overall MAE:    {cv_mae:.2f}s\")\n",
    "print(f\"  Overall RMSE:   {cv_rmse:.2f}s\")\n",
    "print(f\"  Overall R²:     {cv_r2:.4f}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TRAIN ON ALL DATA\n",
    "# -------------------------------------------------------------------------\n",
    "prod_scaler = StandardScaler()\n",
    "X_prod = prod_scaler.fit_transform(X_top)\n",
    "X_prod = np.clip(X_prod, -10, 10)\n",
    "\n",
    "prod_model = best_model_class(**BEST_PARAMS)\n",
    "prod_model.fit(X_prod, y_log)\n",
    "print(f\"Production model trained on full dataset ({len(y_log)} samples).\")\n",
    "print()\n",
    "\n",
    "# Save references for prediction\n",
    "PRODUCTION_FEATURES = TOP_FEATURES\n",
    "PRODUCTION_DROP_COLS = drop_cols\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# SAVE MODEL TO DISK\n",
    "# -------------------------------------------------------------------------\n",
    "model_artifact = {\n",
    "    'model': prod_model,\n",
    "    'scaler': prod_scaler,\n",
    "    'features': PRODUCTION_FEATURES,\n",
    "    'drop_cols': PRODUCTION_DROP_COLS,\n",
    "    'model_name': BEST_MODEL_NAME,\n",
    "    'best_params': BEST_PARAMS,\n",
    "    'best_k': BEST_K,\n",
    "    'cv_metrics': {\n",
    "        'mape': cv_mape,\n",
    "        'medape': cv_medape,\n",
    "        'mae': cv_mae,\n",
    "        'rmse': cv_rmse,\n",
    "        'r2': cv_r2,\n",
    "    },\n",
    "}\n",
    "\n",
    "save_path = Path(\"models/runtime_model.pkl\")\n",
    "joblib.dump(model_artifact, save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(f\"  File size: {save_path.stat().st_size / 1024:.1f} KB\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# VERIFY LOAD WORKS\n",
    "# -------------------------------------------------------------------------\n",
    "loaded = joblib.load(save_path)\n",
    "print(\"Verified model loads successfully.\")\n",
    "print(f\"  Model type: {type(loaded['model']).__name__}\")\n",
    "print(f\"  Features: {len(loaded['features'])}\")\n",
    "print()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PREDICTION FUNCTION\n",
    "# -------------------------------------------------------------------------\n",
    "def predict_runtime(file_path, precision, threshold):\n",
    "    \"\"\"\n",
    "    Predict runtime in seconds for a circuit.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the QASM file\n",
    "        precision: 'single' or 'double'\n",
    "        threshold: The min_threshold value to use\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted runtime in seconds\n",
    "    \"\"\"\n",
    "    features = QASMFeatureExtractor(file_path).extract_all()\n",
    "    features['backend'] = 'CPU'\n",
    "    features['precision'] = precision\n",
    "    features['min_threshold'] = threshold\n",
    "\n",
    "    X = engineer_features(pd.DataFrame([features]))\n",
    "\n",
    "    # Drop non-feature cols\n",
    "    for col in PRODUCTION_DROP_COLS:\n",
    "        if col in X.columns:\n",
    "            X = X.drop(columns=[col])\n",
    "\n",
    "    # One-hot encode\n",
    "    cat = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    X = pd.get_dummies(X, columns=cat)\n",
    "\n",
    "    # Align with training features\n",
    "    for col in PRODUCTION_FEATURES:\n",
    "        if col not in X.columns:\n",
    "            X[col] = 0\n",
    "\n",
    "    X_final = X[PRODUCTION_FEATURES].values.astype(np.float64)\n",
    "    X_final = np.nan_to_num(X_final, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_final = np.clip(prod_scaler.transform(X_final), -10, 10)\n",
    "\n",
    "    return float(np.expm1(prod_model.predict(X_final)[0]))\n",
    "\n",
    "# Quick test on a few training files\n",
    "print(\"=\" * 70)\n",
    "print(\"SANITY CHECK ON TRAINING FILES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "circuits_dir = Path(\"circuits_new\")\n",
    "test_rows = df.sample(n=min(10, len(df)), random_state=42)\n",
    "\n",
    "print(f\"{'File':<45} {'Prec':<7} {'Thresh':>6} {'True':>10} {'Pred':>10} {'Err%':>8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for _, row in test_rows.iterrows():\n",
    "    qasm_path = circuits_dir / row['file']\n",
    "    pred = predict_runtime(qasm_path, row['precision'], int(row['min_threshold']))\n",
    "    true = row['forward_runtime']\n",
    "    err_pct = abs(pred - true) / max(true, 1) * 100\n",
    "    print(f\"{row['file']:<45} {row['precision']:<7} {int(row['min_threshold']):>6} \"\n",
    "          f\"{true:>10.2f} {pred:>10.2f} {err_pct:>7.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"USAGE - Loading saved model\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"  import joblib\")\n",
    "print(\"  artifact = joblib.load('models/runtime_model.pkl')\")\n",
    "print(\"  model = artifact['model']\")\n",
    "print(\"  scaler = artifact['scaler']\")\n",
    "print(\"  features = artifact['features']\")\n",
    "print(\"  cv = artifact['cv_metrics']  # MAPE, MedAPE, MAE, RMSE, R²\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
