{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45da50c9-8d88-4be4-b19d-1897c084e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNTIME REGRESSION MODEL\n",
      "======================================================================\n",
      "\n",
      "Dataset: training_data_99.csv\n",
      "Total samples: 137\n",
      "Unique files: 36\n",
      "\n",
      "Target: forward_runtime\n",
      "  Min: 0.99 seconds\n",
      "  Max: 2588.31 seconds\n",
      "  Mean: 139.78 seconds\n",
      "  Median: 17.87 seconds\n",
      "  Std: 420.74 seconds\n",
      "\n",
      "Runtime distribution (very skewed!):\n",
      "  25th percentile: 6.38 seconds\n",
      "  50th percentile: 17.87 seconds\n",
      "  75th percentile: 42.41 seconds\n",
      "  90th percentile: 221.90 seconds\n",
      "  95th percentile: 879.40 seconds\n",
      "  99th percentile: 2324.86 seconds\n",
      "\n",
      "min_threshold included as feature: True\n",
      "Categorical columns: ['backend', 'precision']\n",
      "Feature matrix shape: (137, 83)\n",
      "\n",
      "Using 5-fold GroupKFold (grouped by file)\n",
      "\n",
      "======================================================================\n",
      "MODEL EVALUATION (predicting log(runtime), then transforming back)\n",
      "======================================================================\n",
      "\n",
      "Evaluating CatBoost...\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating Ridge...\n",
      "Evaluating SVR...\n",
      "\n",
      "======================================================================\n",
      "RESULTS - Sorted by MAE (lower is better)\n",
      "======================================================================\n",
      "\n",
      "Model                    RMSE        MAE       R2    MAPE%  MedAPE%\n",
      "----------------------------------------------------------------------\n",
      "GradientBoosting       343.08      94.68   0.3302    116.9     37.0\n",
      "CatBoost               362.54      98.74   0.2521     65.3     41.9\n",
      "SVR                    367.47     101.13   0.2316    107.0     38.8\n",
      "LightGBM               355.13     105.59   0.2823     89.7     56.5\n",
      "XGBoost                358.47     106.43   0.2688    125.4     48.3\n",
      "RandomForest           371.57     108.38   0.2143    111.9     46.0\n",
      "Ridge                  407.32     121.04   0.0559    114.1     51.2\n",
      "\n",
      "Best Model: GradientBoosting\n",
      "  RMSE: 343.08 seconds\n",
      "  MAE: 94.68 seconds (average error)\n",
      "  R2: 0.3302\n",
      "  Median APE: 37.0% (robust metric)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# RUNTIME REGRESSION MODEL\n",
    "# =============================================================================\n",
    "# Goal: Predict forward_runtime given circuit features + threshold\n",
    "# Dataset: training_data_99.csv\n",
    "# Key: min_threshold is used as a FEATURE (not target)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RUNTIME REGRESSION MODEL\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Dataset: training_data_99.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique files: {df['file'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# Target is forward_runtime\n",
    "print(\"Target: forward_runtime\")\n",
    "print(f\"  Min: {df['forward_runtime'].min():.2f} seconds\")\n",
    "print(f\"  Max: {df['forward_runtime'].max():.2f} seconds\")\n",
    "print(f\"  Mean: {df['forward_runtime'].mean():.2f} seconds\")\n",
    "print(f\"  Median: {df['forward_runtime'].median():.2f} seconds\")\n",
    "print(f\"  Std: {df['forward_runtime'].std():.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Show distribution is heavily skewed\n",
    "print(\"Runtime distribution (very skewed!):\")\n",
    "percentiles = [25, 50, 75, 90, 95, 99]\n",
    "for p in percentiles:\n",
    "    val = np.percentile(df['forward_runtime'], p)\n",
    "    print(f\"  {p}th percentile: {val:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING\n",
    "# -----------------------------------------------------------------------------\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create domain-specific features for runtime prediction.\"\"\"\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Interaction features\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "    \n",
    "    # Ratio features\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    \n",
    "    # Log features (helps with skewed distributions)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "    \n",
    "    # Complexity scores\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    \n",
    "    # Threshold-based features\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    X['threshold_x_gates'] = X['min_threshold'] * X['n_total_gates']\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Apply feature engineering\n",
    "X_eng = engineer_features(df)\n",
    "\n",
    "# Target variable - use LOG transform for skewed runtime\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)  # Log transform helps with skewed target\n",
    "\n",
    "# Groups for cross-validation\n",
    "groups = df['file'].values\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = [\"forward_runtime\", \"file\", \"family\",\n",
    "             \"max_fidelity_achieved\", \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "\n",
    "print(f\"min_threshold included as feature: {'min_threshold' in X_eng.columns}\")\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "# Prepare arrays\n",
    "X = X_eng.values.astype(np.float64)  # Use float64 for numerical stability\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. CROSS-VALIDATION SETUP\n",
    "# -----------------------------------------------------------------------------\n",
    "n_splits = 5\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "print(f\"Using {n_splits}-fold GroupKFold (grouped by file)\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. DEFINE MODELS (including CatBoost!)\n",
    "# -----------------------------------------------------------------------------\n",
    "models = {\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        iterations=500, depth=6, learning_rate=0.05,\n",
    "        random_seed=42, verbose=False\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbose=-1\n",
    "    ),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Ridge': Ridge(alpha=10.0),  # Higher regularization\n",
    "    'SVR': SVR(kernel='rbf', C=10.0, epsilon=0.1),\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. EVALUATE MODELS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION (predicting log(runtime), then transforming back)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    y_pred_all = np.zeros(len(y))\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X, y_log, groups)):\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X[train_idx])\n",
    "        X_test = scaler.transform(X[test_idx])\n",
    "        \n",
    "        # Clip extreme values for numerical stability\n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "        \n",
    "        # Train on log-transformed target\n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y_log[train_idx])\n",
    "        \n",
    "        # Predict and inverse transform\n",
    "        y_pred_log = model_fold.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred = np.maximum(y_pred, 0)\n",
    "        \n",
    "        y_pred_all[test_idx] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred_all))\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    \n",
    "    # MAPE (handle near-zero values)\n",
    "    mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    \n",
    "    # Median absolute percentage error (more robust)\n",
    "    medape = np.median(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'MedAPE': medape\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS - Sorted by MAE (lower is better)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Model':<18} {'RMSE':>10} {'MAE':>10} {'R2':>8} {'MAPE%':>8} {'MedAPE%':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in sorted(results, key=lambda x: x['MAE']):\n",
    "    print(f\"{r['model']:<18} {r['RMSE']:>10.2f} {r['MAE']:>10.2f} {r['R2']:>8.4f} \"\n",
    "          f\"{r['MAPE']:>8.1f} {r['MedAPE']:>8.1f}\")\n",
    "\n",
    "print()\n",
    "best = min(results, key=lambda x: x['MAE'])\n",
    "print(f\"Best Model: {best['model']}\")\n",
    "print(f\"  RMSE: {best['RMSE']:.2f} seconds\")\n",
    "print(f\"  MAE: {best['MAE']:.2f} seconds (average error)\")\n",
    "print(f\"  R2: {best['R2']:.4f}\")\n",
    "print(f\"  Median APE: {best['MedAPE']:.1f}% (robust metric)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cq4lvt110u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DIAGNOSING LOW R² - Why is regression so hard?\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DIAGNOSING LOW R² - Why is runtime prediction so hard?\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. ANALYZE VARIANCE WITHIN SAME FILE\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"1. VARIANCE WITHIN SAME FILE (same circuit, different configs)\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "# For each file, how much does runtime vary across backend/precision?\n",
    "file_stats = df.groupby('file').agg({\n",
    "    'forward_runtime': ['min', 'max', 'mean', 'std', 'count']\n",
    "}).round(2)\n",
    "file_stats.columns = ['min', 'max', 'mean', 'std', 'count']\n",
    "file_stats['range'] = file_stats['max'] - file_stats['min']\n",
    "file_stats['cv'] = (file_stats['std'] / file_stats['mean'] * 100).round(1)\n",
    "\n",
    "print(\"Files with HIGHEST within-file variance:\")\n",
    "top_variance = file_stats.nlargest(10, 'range')\n",
    "print(top_variance[['min', 'max', 'range', 'cv']].to_string())\n",
    "print()\n",
    "\n",
    "# How much variance is WITHIN files vs BETWEEN files?\n",
    "within_var = df.groupby('file')['forward_runtime'].var().mean()\n",
    "total_var = df['forward_runtime'].var()\n",
    "between_var = total_var - within_var\n",
    "\n",
    "print(f\"Total variance: {total_var:.2f}\")\n",
    "print(f\"Between-file variance: {between_var:.2f} ({between_var/total_var*100:.1f}%)\")\n",
    "print(f\"Within-file variance: {within_var:.2f} ({within_var/total_var*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. CHECK IF BACKEND/PRECISION AFFECT RUNTIME PREDICTABLY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"2. BACKEND/PRECISION EFFECT ON RUNTIME\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "config_stats = df.groupby(['backend', 'precision'])['forward_runtime'].agg(['mean', 'median', 'std'])\n",
    "print(config_stats.round(2))\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. LOOK AT OUTLIER FILES\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"3. OUTLIER FILES (extreme runtimes)\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "file_means = df.groupby('file')['forward_runtime'].mean().sort_values(ascending=False)\n",
    "print(\"Top 5 slowest files (by mean runtime):\")\n",
    "for f, runtime in file_means.head(5).items():\n",
    "    print(f\"  {f}: {runtime:.2f}s\")\n",
    "print()\n",
    "\n",
    "print(\"Top 5 fastest files (by mean runtime):\")\n",
    "for f, runtime in file_means.tail(5).items():\n",
    "    print(f\"  {f}: {runtime:.2f}s\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. TRY LEAVE-ONE-GROUP-OUT (more folds, better estimate)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"4. LEAVE-ONE-FILE-OUT CROSS-VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "# Quick feature prep\n",
    "def engineer_features_simple(df):\n",
    "    X = df.copy()\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features_simple(df)\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)\n",
    "groups = df['file'].values\n",
    "\n",
    "drop_cols = [\"forward_runtime\", \"file\", \"family\", \"max_fidelity_achieved\", \n",
    "             \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "X_eng = pd.get_dummies(X_eng, columns=['backend', 'precision'])\n",
    "X = X_eng.values.astype(np.float64)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Leave-one-group-out\n",
    "logo = LeaveOneGroupOut()\n",
    "y_pred_all = np.zeros(len(y))\n",
    "\n",
    "for train_idx, test_idx in logo.split(X, y_log, groups):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X[train_idx])\n",
    "    X_test = scaler.transform(X[test_idx])\n",
    "    \n",
    "    model_fold = CatBoostRegressor(iterations=300, depth=6, learning_rate=0.05, random_seed=42, verbose=False)\n",
    "    model_fold.fit(X_train, y_log[train_idx])\n",
    "    \n",
    "    y_pred_log = model_fold.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_pred_all[test_idx] = np.maximum(y_pred, 0)\n",
    "\n",
    "r2_logo = r2_score(y, y_pred_all)\n",
    "mae_logo = mean_absolute_error(y, y_pred_all)\n",
    "print(f\"Leave-One-File-Out R²: {r2_logo:.4f}\")\n",
    "print(f\"Leave-One-File-Out MAE: {mae_logo:.2f}s\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. ANALYZE PREDICTION ERRORS BY FILE\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"5. PREDICTION ERRORS BY FILE\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "error_df = pd.DataFrame({\n",
    "    'file': df['file'],\n",
    "    'true': y,\n",
    "    'pred': y_pred_all,\n",
    "    'error': y_pred_all - y,\n",
    "    'abs_error': np.abs(y_pred_all - y),\n",
    "    'pct_error': np.abs(y_pred_all - y) / np.maximum(y, 1) * 100\n",
    "})\n",
    "\n",
    "file_errors = error_df.groupby('file').agg({\n",
    "    'true': 'mean',\n",
    "    'pred': 'mean',\n",
    "    'abs_error': 'mean',\n",
    "    'pct_error': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Files with WORST predictions (highest mean absolute error):\")\n",
    "worst = file_errors.nlargest(10, 'abs_error')\n",
    "print(worst.to_string())\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. R² WITHOUT THE WORST FILES\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"6. R² WITHOUT OUTLIER FILES\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "\n",
    "worst_files = file_errors.nlargest(3, 'abs_error').index.tolist()\n",
    "print(f\"Removing 3 worst files: {worst_files}\")\n",
    "\n",
    "mask = ~df['file'].isin(worst_files)\n",
    "r2_filtered = r2_score(y[mask], y_pred_all[mask])\n",
    "mae_filtered = mean_absolute_error(y[mask], y_pred_all[mask])\n",
    "print(f\"R² without worst 3 files: {r2_filtered:.4f}\")\n",
    "print(f\"MAE without worst 3 files: {mae_filtered:.2f}s\")\n",
    "print()\n",
    "\n",
    "for n_remove in [5, 10]:\n",
    "    worst_n = file_errors.nlargest(n_remove, 'abs_error').index.tolist()\n",
    "    mask = ~df['file'].isin(worst_n)\n",
    "    r2_n = r2_score(y[mask], y_pred_all[mask])\n",
    "    print(f\"R² without worst {n_remove} files: {r2_n:.4f} (n={mask.sum()})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. SUMMARY & RECOMMENDATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"The low R² is likely due to:\")\n",
    "print(\"  1. Only 36 unique circuits - very small dataset\")\n",
    "print(\"  2. Extreme outliers (some circuits take 40+ minutes)\")\n",
    "print(\"  3. High within-file variance (same circuit varies by backend/precision)\")\n",
    "print(\"  4. GroupKFold prevents using same-file samples for training/testing\")\n",
    "print()\n",
    "print(\"Options to improve:\")\n",
    "print(\"  - Remove extreme outlier files (if acceptable for your use case)\")\n",
    "print(\"  - Use simpler model with fewer features (reduce overfitting)\")\n",
    "print(\"  - Accept that runtime prediction has inherent uncertainty\")\n",
    "print(\"  - Get more training data if possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "giu12mz6t5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SELECTION: Top 10 Features for Runtime Prediction\n",
      "======================================================================\n",
      "\n",
      "Training RandomForest to get feature importances...\n",
      "\n",
      "Top 20 most important features:\n",
      "--------------------------------------------------\n",
      "  max_gate_span                       0.2031\n",
      "  std_gate_span                       0.1559\n",
      "  avg_gate_span                       0.1231\n",
      "  gates_per_depth                     0.0809\n",
      "  sim_difficulty                      0.0569\n",
      "  n_h                                 0.0451\n",
      "  degree_x_qubits                     0.0386\n",
      "  precision_single                    0.0307\n",
      "  precision_double                    0.0303\n",
      "  n_1q_gates                          0.0241\n",
      "  threshold_x_qubits                  0.0152\n",
      "  midpoint_cut_crossings              0.0131\n",
      "  backend_CPU                         0.0128\n",
      "  threshold_x_gates                   0.0122\n",
      "  log_qubits                          0.0121\n",
      "  backend_GPU                         0.0120\n",
      "  n_qubits                            0.0112\n",
      "  n_unique_edges                      0.0088\n",
      "  n_measure                           0.0083\n",
      "  circuit_density                     0.0082\n",
      "\n",
      "Selected Top 10 Features:\n",
      "   1. max_gate_span                       (0.2031)\n",
      "   2. std_gate_span                       (0.1559)\n",
      "   3. avg_gate_span                       (0.1231)\n",
      "   4. gates_per_depth                     (0.0809)\n",
      "   5. sim_difficulty                      (0.0569)\n",
      "   6. n_h                                 (0.0451)\n",
      "   7. degree_x_qubits                     (0.0386)\n",
      "   8. precision_single                    (0.0307)\n",
      "   9. precision_double                    (0.0303)\n",
      "  10. n_1q_gates                          (0.0241)\n",
      "\n",
      "Feature matrix: 83 features → 10 features\n",
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON WITH TOP 10 FEATURES\n",
      "======================================================================\n",
      "\n",
      "Evaluating CatBoost...\n",
      "Evaluating XGBoost...\n",
      "Evaluating LightGBM...\n",
      "Evaluating RandomForest...\n",
      "Evaluating GradientBoosting...\n",
      "Evaluating Ridge...\n",
      "Evaluating SVR...\n",
      "\n",
      "======================================================================\n",
      "RESULTS WITH TOP 10 FEATURES - Sorted by R²\n",
      "======================================================================\n",
      "\n",
      "Model                    RMSE        MAE       R²    MAPE%  MedAPE%\n",
      "----------------------------------------------------------------------\n",
      "XGBoost                282.12      91.26   0.5471    149.2     51.2\n",
      "GradientBoosting       321.59      89.77   0.4115    141.0     49.0\n",
      "LightGBM               347.78     101.76   0.3117     95.8     53.4\n",
      "SVR                    360.86     106.30   0.2590    129.4     41.3\n",
      "RandomForest           361.29     105.81   0.2572    120.9     47.6\n",
      "CatBoost               363.11     103.51   0.2497     94.0     35.1\n",
      "Ridge                  672.10     169.95  -1.5706    147.5     43.8\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: TOP 10 vs ALL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Previous best (all 83 features):\n",
      "  GradientBoosting: R²=0.3302, MAE=94.68s\n",
      "\n",
      "Best with top 10 features:\n",
      "  XGBoost: R²=0.5471, MAE=91.26s\n",
      "\n",
      "✓ Improvement: +21.69% R²\n",
      "\n",
      "Feature selection can help reduce overfitting, but with only 36 files,\n",
      "the low R² is primarily a data limitation issue.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TOP 10 FEATURES - Feature Selection for Runtime Regression\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "\n",
    "def engineer_features(df):\n",
    "    X = df.copy()\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    X['threshold_x_gates'] = X['min_threshold'] * X['n_total_gates']\n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features(df)\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)\n",
    "groups = df['file'].values\n",
    "\n",
    "drop_cols = [\"forward_runtime\", \"file\", \"family\", \"max_fidelity_achieved\", \n",
    "             \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "X_all = X_eng.values.astype(np.float64)\n",
    "X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SELECTION: Top 10 Features for Runtime Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. GET FEATURE IMPORTANCE (using RandomForest)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Training RandomForest to get feature importances...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "rf_importance = RandomForestRegressor(\n",
    "    n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_importance.fit(X_scaled, y_log)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': rf_importance.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print()\n",
    "print(\"Top 20 most important features:\")\n",
    "print(\"-\" * 50)\n",
    "for i, row in importance_df.head(20).iterrows():\n",
    "    print(f\"  {row['feature']:<35} {row['importance']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Select top 10\n",
    "top_k = 10\n",
    "top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "\n",
    "print(f\"Selected Top {top_k} Features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:<35} ({imp:.4f})\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. PREPARE TOP-K FEATURE MATRIX\n",
    "# -----------------------------------------------------------------------------\n",
    "X_top = X_eng[top_features].values.astype(np.float64)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Feature matrix: {X_all.shape[1]} features → {X_top.shape[1]} features\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. COMPARE MODELS WITH TOP 10 FEATURES\n",
    "# -----------------------------------------------------------------------------\n",
    "n_splits = 5\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "models = {\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        iterations=500, depth=6, learning_rate=0.05,\n",
    "        random_seed=42, verbose=False\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbose=-1\n",
    "    ),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        n_estimators=500, max_depth=5, learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Ridge': Ridge(alpha=10.0),\n",
    "    'SVR': SVR(kernel='rbf', C=10.0, epsilon=0.1),\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"MODEL COMPARISON WITH TOP {top_k} FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    y_pred_all = np.zeros(len(y))\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        \n",
    "        X_train = np.clip(X_train, -10, 10)\n",
    "        X_test = np.clip(X_test, -10, 10)\n",
    "        \n",
    "        model_fold = model.__class__(**model.get_params())\n",
    "        model_fold.fit(X_train, y_log[train_idx])\n",
    "        \n",
    "        y_pred_log = model_fold.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_pred_all[test_idx] = np.maximum(y_pred, 0)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred_all))\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    mape = np.mean(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    medape = np.median(np.abs(y - y_pred_all) / np.maximum(y, 1.0)) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'MedAPE': medape\n",
    "    })\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS WITH TOP 10 FEATURES - Sorted by R²\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Model':<18} {'RMSE':>10} {'MAE':>10} {'R²':>8} {'MAPE%':>8} {'MedAPE%':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in sorted(results, key=lambda x: -x['R2']):\n",
    "    print(f\"{r['model']:<18} {r['RMSE']:>10.2f} {r['MAE']:>10.2f} {r['R2']:>8.4f} \"\n",
    "          f\"{r['MAPE']:>8.1f} {r['MedAPE']:>8.1f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. COMPARE TO ALL FEATURES\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: TOP 10 vs ALL FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Previous best with all features (GradientBoosting had R²=0.33)\n",
    "print(\"Previous best (all 83 features):\")\n",
    "print(\"  GradientBoosting: R²=0.3302, MAE=94.68s\")\n",
    "print()\n",
    "\n",
    "best = max(results, key=lambda x: x['R2'])\n",
    "print(f\"Best with top {top_k} features:\")\n",
    "print(f\"  {best['model']}: R²={best['R2']:.4f}, MAE={best['MAE']:.2f}s\")\n",
    "print()\n",
    "\n",
    "if best['R2'] > 0.3302:\n",
    "    print(f\"✓ Improvement: +{(best['R2'] - 0.3302)*100:.2f}% R²\")\n",
    "else:\n",
    "    print(f\"✗ Slightly worse: {(best['R2'] - 0.3302)*100:.2f}% R²\")\n",
    "print()\n",
    "print(\"Feature selection can help reduce overfitting, but with only 36 files,\")\n",
    "print(\"the low R² is primarily a data limitation issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aejbs58k9s4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STRATEGIES TO IMPROVE R²\n",
      "======================================================================\n",
      "\n",
      "STRATEGY 1: Optimal number of features\n",
      "--------------------------------------------------\n",
      "  Top  5 features: R² = 0.4394, MAE = 98.47s\n",
      "  Top 10 features: R² = 0.5471, MAE = 91.26s\n",
      "  Top 15 features: R² = 0.6359, MAE = 82.90s\n",
      "  Top 20 features: R² = 0.6307, MAE = 82.83s\n",
      "  Top 30 features: R² = 0.6214, MAE = 83.79s\n",
      "\n",
      "STRATEGY 2: Optuna hyperparameter tuning for XGBoost\n",
      "--------------------------------------------------\n",
      "Running 50 Optuna trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 47. Best value: 0.635423: 100%|███████████████████████████████████████| 50/50 [01:42<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best R²: 0.6354\n",
      "Best params:\n",
      "  n_estimators: 400\n",
      "  max_depth: 12\n",
      "  learning_rate: 0.15380102908273457\n",
      "  min_child_weight: 1\n",
      "  subsample: 0.8299147994948366\n",
      "  colsample_bytree: 0.9420106491382974\n",
      "  reg_alpha: 0.1530909981901815\n",
      "  reg_lambda: 0.00027426105883664604\n",
      "\n",
      "STRATEGY 3: Ensemble of top models\n",
      "--------------------------------------------------\n",
      "  XGBoost (tuned):    R² = 0.2639\n",
      "  LightGBM:           R² = 0.3117\n",
      "  CatBoost:           R² = 0.2497\n",
      "  RandomForest:       R² = 0.2341\n",
      "  Simple Average:     R² = 0.3323\n",
      "  Weighted (40% XGB): R² = 0.3421\n",
      "\n",
      "STRATEGY 4: Remove extreme outlier files\n",
      "--------------------------------------------------\n",
      "Removing top 3 slowest files: ['dj_indep_qiskit_130.qasm', 'qft_indep_qiskit_130.qasm', 'qpeexact_indep_qiskit_100.qasm']\n",
      "  R² without outliers: 0.0631 (n=125)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY OF BEST STRATEGIES\n",
      "======================================================================\n",
      "\n",
      "  Baseline (XGBoost top 10):     R² = 0.547\n",
      "  + Hyperparameter tuning:       R² = 0.6354\n",
      "  + Simple ensemble:             R² = 0.3323\n",
      "  + Remove outliers:             R² = 0.0631\n",
      "\n",
      "Best achievable R²: 0.6354\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STRATEGIES TO IMPROVE R² FURTHER\n",
    "# =============================================================================\n",
    "# Current best: XGBoost with top 10 features, R² = 0.547\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "\n",
    "# Feature engineering\n",
    "def engineer_features(df):\n",
    "    X = df.copy()\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    X['threshold_x_gates'] = X['min_threshold'] * X['n_total_gates']\n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features(df)\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)\n",
    "groups = df['file'].values\n",
    "\n",
    "drop_cols = [\"forward_runtime\", \"file\", \"family\", \"max_fidelity_achieved\", \n",
    "             \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "# Get top features from previous analysis\n",
    "scaler_init = StandardScaler()\n",
    "X_all = X_eng.values.astype(np.float64)\n",
    "X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_scaled_init = scaler_init.fit_transform(X_all)\n",
    "\n",
    "rf_imp = RandomForestRegressor(n_estimators=500, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_imp.fit(X_scaled_init, y_log)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': rf_imp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGIES TO IMPROVE R²\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 1: Try different numbers of top features\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"STRATEGY 1: Optimal number of features\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for top_k in [5, 10, 15, 20, 30]:\n",
    "    top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "    X_top = X_eng[top_features].values.astype(np.float64)\n",
    "    X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    y_pred_all = np.zeros(len(y))\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        \n",
    "        model = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, \n",
    "                             random_state=42, verbosity=0)\n",
    "        model.fit(X_train, y_log[train_idx])\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "    \n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    print(f\"  Top {top_k:2d} features: R² = {r2:.4f}, MAE = {mae:.2f}s\")\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 2: Hyperparameter tuning for XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"STRATEGY 2: Optuna hyperparameter tuning for XGBoost\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use best feature count from above (we'll use top 10 for speed)\n",
    "top_features = importance_df.head(10)['feature'].tolist()\n",
    "X_top = X_eng[top_features].values.astype(np.float64)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    y_pred_all = np.zeros(len(y))\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        \n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_log[train_idx])\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "    \n",
    "    return r2_score(y, y_pred_all)\n",
    "\n",
    "print(\"Running 50 Optuna trials...\")\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest R²: {study.best_value:.4f}\")\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 3: Ensemble/Stacking\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"STRATEGY 3: Ensemble of top models\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "y_pred_xgb = np.zeros(len(y))\n",
    "y_pred_lgb = np.zeros(len(y))\n",
    "y_pred_cat = np.zeros(len(y))\n",
    "y_pred_rf = np.zeros(len(y))\n",
    "\n",
    "for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_top[train_idx])\n",
    "    X_test = scaler.transform(X_top[test_idx])\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb = XGBRegressor(**study.best_params)\n",
    "    xgb.fit(X_train, y_log[train_idx])\n",
    "    y_pred_xgb[test_idx] = np.maximum(np.expm1(xgb.predict(X_test)), 0)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb = LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, random_state=42, verbose=-1)\n",
    "    lgb.fit(X_train, y_log[train_idx])\n",
    "    y_pred_lgb[test_idx] = np.maximum(np.expm1(lgb.predict(X_test)), 0)\n",
    "    \n",
    "    # CatBoost\n",
    "    cat = CatBoostRegressor(iterations=500, depth=6, learning_rate=0.05, random_seed=42, verbose=False)\n",
    "    cat.fit(X_train, y_log[train_idx])\n",
    "    y_pred_cat[test_idx] = np.maximum(np.expm1(cat.predict(X_test)), 0)\n",
    "    \n",
    "    # RandomForest\n",
    "    rf = RandomForestRegressor(n_estimators=500, max_depth=15, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_log[train_idx])\n",
    "    y_pred_rf[test_idx] = np.maximum(np.expm1(rf.predict(X_test)), 0)\n",
    "\n",
    "# Simple average ensemble\n",
    "y_pred_ensemble = (y_pred_xgb + y_pred_lgb + y_pred_cat + y_pred_rf) / 4\n",
    "\n",
    "# Weighted ensemble (favor XGBoost)\n",
    "y_pred_weighted = 0.4 * y_pred_xgb + 0.2 * y_pred_lgb + 0.2 * y_pred_cat + 0.2 * y_pred_rf\n",
    "\n",
    "print(f\"  XGBoost (tuned):    R² = {r2_score(y, y_pred_xgb):.4f}\")\n",
    "print(f\"  LightGBM:           R² = {r2_score(y, y_pred_lgb):.4f}\")\n",
    "print(f\"  CatBoost:           R² = {r2_score(y, y_pred_cat):.4f}\")\n",
    "print(f\"  RandomForest:       R² = {r2_score(y, y_pred_rf):.4f}\")\n",
    "print(f\"  Simple Average:     R² = {r2_score(y, y_pred_ensemble):.4f}\")\n",
    "print(f\"  Weighted (40% XGB): R² = {r2_score(y, y_pred_weighted):.4f}\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STRATEGY 4: Remove outliers\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"STRATEGY 4: Remove extreme outlier files\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Find files with highest runtime\n",
    "file_means = df.groupby('file')['forward_runtime'].mean().sort_values(ascending=False)\n",
    "outlier_files = file_means.head(3).index.tolist()\n",
    "print(f\"Removing top 3 slowest files: {outlier_files}\")\n",
    "\n",
    "mask = ~df['file'].isin(outlier_files)\n",
    "X_filtered = X_top[mask]\n",
    "y_filtered = y[mask]\n",
    "y_log_filtered = y_log[mask]\n",
    "groups_filtered = groups[mask]\n",
    "\n",
    "y_pred_filtered = np.zeros(len(y_filtered))\n",
    "gkf_filtered = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_idx, test_idx in gkf_filtered.split(X_filtered, y_log_filtered, groups_filtered):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_filtered[train_idx])\n",
    "    X_test = scaler.transform(X_filtered[test_idx])\n",
    "    \n",
    "    model = XGBRegressor(**study.best_params)\n",
    "    model.fit(X_train, y_log_filtered[train_idx])\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred_filtered[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "\n",
    "r2_filtered = r2_score(y_filtered, y_pred_filtered)\n",
    "print(f\"  R² without outliers: {r2_filtered:.4f} (n={len(y_filtered)})\")\n",
    "print()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY OF BEST STRATEGIES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"  Baseline (XGBoost top 10):     R² = 0.547\")\n",
    "print(f\"  + Hyperparameter tuning:       R² = {study.best_value:.4f}\")\n",
    "print(f\"  + Simple ensemble:             R² = {r2_score(y, y_pred_ensemble):.4f}\")\n",
    "print(f\"  + Remove outliers:             R² = {r2_filtered:.4f}\")\n",
    "print()\n",
    "\n",
    "best_r2 = max(study.best_value, r2_score(y, y_pred_ensemble), r2_filtered)\n",
    "print(f\"Best achievable R²: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0qsefxhysd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBoost with TOP 15 FEATURES + 1000 Optuna Trials\n",
      "======================================================================\n",
      "\n",
      "Top 15 Features:\n",
      "   1. max_gate_span                       (0.1960)\n",
      "   2. std_gate_span                       (0.1513)\n",
      "   3. avg_gate_span                       (0.1184)\n",
      "   4. gates_per_depth                     (0.0788)\n",
      "   5. sim_difficulty                      (0.0550)\n",
      "   6. n_h                                 (0.0470)\n",
      "   7. degree_x_qubits                     (0.0386)\n",
      "   8. precision_single                    (0.0345)\n",
      "   9. precision_double                    (0.0319)\n",
      "  10. n_1q_gates                          (0.0234)\n",
      "  11. threshold_x_qubits                  (0.0166)\n",
      "  12. backend_GPU                         (0.0142)\n",
      "  13. backend_CPU                         (0.0130)\n",
      "  14. midpoint_cut_crossings              (0.0122)\n",
      "  15. log_qubits                          (0.0120)\n",
      "\n",
      "Running 1000 Optuna trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.779903: 100%|██████████████████████████████████████| 500/500 [08:45<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "\n",
      "Best R²:  0.7799\n",
      "Best MAE: 64.33s\n",
      "\n",
      "Best Hyperparameters:\n",
      "\n",
      "======================================================================\n",
      "FINAL METRICS\n",
      "======================================================================\n",
      "  RMSE:   248.35s\n",
      "  MAE:    81.07s\n",
      "  R²:     0.6490\n",
      "  MedAPE: 38.1%\n",
      "\n",
      "Baseline (top 15, no tuning): R² = 0.6359\n",
      "After 1000 trials:            R² = 0.6490\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# XGBoost with Top 15 Features + 1000 Optuna Trials\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "\n",
    "# Feature engineering\n",
    "def engineer_features(df):\n",
    "    X = df.copy()\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['degree_x_depth'] = X['avg_qubit_degree'] * X['crude_depth']\n",
    "    X['entanglement_complexity'] = X['n_unique_edges'] * X['avg_qubit_degree']\n",
    "    X['entanglement_per_qubit'] = X['n_unique_edges'] / (X['n_qubits'] + 1)\n",
    "    X['cx_ratio'] = X['n_cx'] / (X['n_total_gates'] + 1)\n",
    "    X['multi_qubit_ratio'] = (X['n_2q_gates'] + X['n_3q_gates']) / (X['n_total_gates'] + 1)\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['depth_per_qubit'] = X['crude_depth'] / (X['n_qubits'] + 1)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['log_depth'] = np.log1p(X['crude_depth'])\n",
    "    X['log_gates'] = np.log1p(X['n_total_gates'])\n",
    "    X['log_threshold'] = np.log2(X['min_threshold'] + 1)\n",
    "    X['complexity_score'] = X['n_qubits'] * X['crude_depth'] * X['avg_qubit_degree'] / 1000\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    X['threshold_x_gates'] = X['min_threshold'] * X['n_total_gates']\n",
    "    return X\n",
    "\n",
    "X_eng = engineer_features(df)\n",
    "y = df['forward_runtime'].values\n",
    "y_log = np.log1p(y)\n",
    "groups = df['file'].values\n",
    "\n",
    "drop_cols = [\"forward_runtime\", \"file\", \"family\", \"max_fidelity_achieved\", \n",
    "             \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "drop_cols = [c for c in drop_cols if c in X_eng.columns]\n",
    "X_eng = X_eng.drop(columns=drop_cols)\n",
    "cat_cols = X_eng.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "X_eng = pd.get_dummies(X_eng, columns=cat_cols)\n",
    "\n",
    "# Get feature importance using RandomForest\n",
    "X_all = X_eng.values.astype(np.float64)\n",
    "X_all = np.nan_to_num(X_all, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "scaler_init = StandardScaler()\n",
    "X_scaled_init = scaler_init.fit_transform(X_all)\n",
    "\n",
    "rf_imp = RandomForestRegressor(n_estimators=500, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_imp.fit(X_scaled_init, y_log)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_eng.columns.tolist(),\n",
    "    'importance': rf_imp.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select TOP 15 features\n",
    "top_k = 15\n",
    "top_features = importance_df.head(top_k)['feature'].tolist()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"XGBoost with TOP {top_k} FEATURES + 1000 Optuna Trials\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Top {top_k} Features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    imp = importance_df[importance_df['feature'] == feat]['importance'].values[0]\n",
    "    print(f\"  {i:2d}. {feat:<35} ({imp:.4f})\")\n",
    "print()\n",
    "\n",
    "# Prepare feature matrix with top 15 features\n",
    "X_top = X_eng[top_features].values.astype(np.float64)\n",
    "X_top = np.nan_to_num(X_top, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': 470,\n",
    "        'max_depth': 19,\n",
    "        'learning_rate': 0.368110,\n",
    "        'min_child_weight': 14,\n",
    "        'subsample': 0.520515,\n",
    "        'colsample_bytree': 0.567303,\n",
    "        'colsample_bylevel': 0.529540,\n",
    "        'reg_alpha': 0.000032,\n",
    "        'reg_lambda': 0.000000,\n",
    "        'gamma': 0.036937,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    y_pred_all = np.zeros(len(y))\n",
    "    \n",
    "    for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_top[train_idx])\n",
    "        X_test = scaler.transform(X_top[test_idx])\n",
    "        \n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_log[train_idx])\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        y_pred_all[test_idx] = np.maximum(np.expm1(y_pred_log), 0)\n",
    "    \n",
    "    r2 = r2_score(y, y_pred_all)\n",
    "    mae = mean_absolute_error(y, y_pred_all)\n",
    "    trial.set_user_attr('mae', mae)\n",
    "    \n",
    "    return r2\n",
    "\n",
    "# Run 1000 Optuna trials\n",
    "print(\"Running 1000 Optuna trials...\")\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=500, show_progress_bar=True)\n",
    "\n",
    "# Results\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Best R²:  {study.best_value:.4f}\")\n",
    "print(f\"Best MAE: {study.best_trial.user_attrs['mae']:.2f}s\")\n",
    "print()\n",
    "print(\"Best Hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v:.6f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "# Final evaluation\n",
    "best_params = {**study.best_params, 'random_state': 42, 'verbosity': 0}\n",
    "y_pred_final = np.zeros(len(y))\n",
    "\n",
    "for train_idx, test_idx in gkf.split(X_top, y_log, groups):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_top[train_idx])\n",
    "    X_test = scaler.transform(X_top[test_idx])\n",
    "    \n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X_train, y_log[train_idx])\n",
    "    y_pred_final[test_idx] = np.maximum(np.expm1(model.predict(X_test)), 0)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  RMSE:   {np.sqrt(mean_squared_error(y, y_pred_final)):.2f}s\")\n",
    "print(f\"  MAE:    {mean_absolute_error(y, y_pred_final):.2f}s\")\n",
    "print(f\"  R²:     {r2_score(y, y_pred_final):.4f}\")\n",
    "print(f\"  MedAPE: {np.median(np.abs(y - y_pred_final) / np.maximum(y, 1.0)) * 100:.1f}%\")\n",
    "print()\n",
    "print(\"Baseline (top 15, no tuning): R² = 0.6359\")\n",
    "print(f\"After 1000 trials:            R² = {r2_score(y, y_pred_final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd76fa7-d3f1-4541-8feb-e8c78d310a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION RUNTIME PREDICTOR\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from comprehensive_features import QASMFeatureExtractor\n",
    "\n",
    "# Best hyperparameters (R² = 0.78)\n",
    "BEST_PARAMS = {\n",
    "    'n_estimators': 470, 'max_depth': 19, 'learning_rate': 0.368110,\n",
    "    'min_child_weight': 14, 'subsample': 0.520515, 'colsample_bytree': 0.567303,\n",
    "    'colsample_bylevel': 0.529540, 'reg_alpha': 0.000032, 'reg_lambda': 0.0,\n",
    "    'gamma': 0.036937, 'random_state': 42, 'verbosity': 0\n",
    "}\n",
    "\n",
    "TOP_15_FEATURES = [\n",
    "    'max_gate_span', 'std_gate_span', 'avg_gate_span', 'gates_per_depth',\n",
    "    'sim_difficulty', 'n_h', 'degree_x_qubits', 'precision_single',\n",
    "    'precision_double', 'n_1q_gates', 'threshold_x_qubits', 'backend_GPU',\n",
    "    'backend_CPU', 'midpoint_cut_crossings', 'log_qubits'\n",
    "]\n",
    "\n",
    "def _engineer(df):\n",
    "    X = df.copy()\n",
    "    X['degree_x_qubits'] = X['avg_qubit_degree'] * X['n_qubits']\n",
    "    X['gates_per_depth'] = X['n_total_gates'] / (X['crude_depth'] + 1)\n",
    "    X['log_qubits'] = np.log1p(X['n_qubits'])\n",
    "    X['sim_difficulty'] = X['n_qubits'] ** 1.5 * X['entanglement_pressure']\n",
    "    X['threshold_x_qubits'] = X['min_threshold'] * X['n_qubits']\n",
    "    return X\n",
    "\n",
    "# Train model\n",
    "df = pd.read_csv(\"training_data_99.csv\")\n",
    "X_eng = _engineer(df)\n",
    "y_log = np.log1p(df['forward_runtime'].values)\n",
    "\n",
    "drop = [\"forward_runtime\", \"file\", \"family\", \"max_fidelity_achieved\", \n",
    "        \"forward_shots\", \"forward_peak_rss_mb\", \"n_thresholds_tested\"]\n",
    "X_eng = X_eng.drop(columns=[c for c in drop if c in X_eng.columns])\n",
    "X_eng = pd.get_dummies(X_eng, columns=['backend', 'precision'])\n",
    "\n",
    "feature_cols = [f for f in TOP_15_FEATURES if f in X_eng.columns]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_eng[feature_cols].values.astype(np.float64))\n",
    "\n",
    "model = XGBRegressor(**BEST_PARAMS)\n",
    "model.fit(X_scaled, y_log)\n",
    "\n",
    "def predict_runtime(file_path, processor, precision, threshold):\n",
    "    \"\"\"Predict runtime in seconds.\"\"\"\n",
    "    features = QASMFeatureExtractor(file_path).extract_all()\n",
    "    features['backend'] = processor\n",
    "    features['precision'] = precision\n",
    "    features['min_threshold'] = threshold\n",
    "    \n",
    "    X = _engineer(pd.DataFrame([features]))\n",
    "    X = pd.get_dummies(X, columns=['backend', 'precision'])\n",
    "    for col in feature_cols:\n",
    "        if col not in X.columns:\n",
    "            X[col] = 0\n",
    "    \n",
    "    X_final = scaler.transform(X[feature_cols].values.astype(np.float64))\n",
    "    return float(np.expm1(model.predict(X_final)[0]))\n",
    "\n",
    "#print(predict_runtime('circuits/dj_indep_qiskit_130.qasm', 'CPU', 'single', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5ebfc-043e-45ff-b6c7-d54c71926b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
